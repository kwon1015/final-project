{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 흐름을 생각해보자\n",
    "\n",
    "BM25로 문장을 embedding한 후에 각 vector들에 대한 self.embedding과 keyword의 embedding \n",
    "행렬곱을 normalization 한 후에 가장 높은 것 + 일정 수치 이상\n",
    "\n",
    "1. normalization 값을 어떻게 할 것인가\n",
    " - 각 키워드에 대한 행렬곱을 살펴보고 어느 정도 수치가 나오는지 정성적 평가 -> threshold 정하기\n",
    " - 문장이 \"모르겠어요.\" 혹은 얼토당토하지 않은 문장을 적었음에도 top 순위로 하면 키워드를 필수적으로 찾아야하므로 오히려 오답을 매칭하는 경우가 있을 것\n",
    " - 그렇다고 threshold를 너무 높이다보면 오히려 EM으로 찾을 수 있는 단어들만이 출현할 것\n",
    " - 결국 정성적인 평가밖에 방식이 없나 싶다.\n",
    "\n",
    "2. 일정 수치는 어떻게 정할 것인가\n",
    "\n",
    "Dense Retriever의 경우 우리의 키워드가 문장의 어떤 단어와 연관이 높은지를 학습시켜줄 label이 필요하다\n",
    "ex) 기온 - 온도가 연관이 있다는 label 필요\n",
    "하지만 현재 그러한 데이터셋이 존재하지 않는 상황이기 때문에 이를 위한 작업은 너무 많은 시간을 소요하게 될 것 같다.\n",
    "따라서 이는 제외하는 것이 맞아보인다.\n",
    "\n",
    "결과적으로 BM25로만의 embedding 이후에 결과를 살펴봐야 할 것 같다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "import rank_bm25\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softmax\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, BertModel, BertPreTrainedModel, AdamW, TrainingArguments, get_linear_schedule_with_warmup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "class MyBm25(rank_bm25.BM25Okapi): \n",
    "                                                                      \n",
    "    def __init__(self, corpus, tokenizer=None, k1=1.5, b=0.75, epsilon=0.25):\n",
    "        # 논문에 따르면 k1은 1.2~1.5가 적당하다. 또한 b의 경우에도 0.75~0.9가 적당하다.\n",
    "        # GridSearchCV를 통해서 최적의 값을 찾아보고 정의하는 것이 좋을 듯 하다.\n",
    "            super().__init__(corpus, tokenizer=tokenizer, k1=k1, b=b, epsilon=epsilon)    \n",
    "    \n",
    "    def get_top_n(self, query, documents, n=5):\n",
    "        assert self.corpus_size == len(documents), \"The documents given don't match the index corpus!\"\n",
    "\n",
    "        scores = self.get_scores(query)\n",
    "        # 이미 구현되어 있는 함수를 사용하여 점수를 구한다.\n",
    "\n",
    "        top_n_idx = np.argsort(scores)[::-1][:n]\n",
    "        doc_score = scores[top_n_idx]\n",
    "        \n",
    "        return doc_score, top_n_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"../data/\" + \"KorSTS/sts-train.tsv\", sep='\\t')\n",
    "contexts1 = list(\n",
    "            df.iloc[i].sentence1 for i in range(len(df))\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "contexts2 = list(df.iloc[j].sentence2 for j in range(len(df)))\n",
    "contexts = contexts1 + contexts2\n",
    "for idx, i in enumerate(contexts):\n",
    "    if type(i) == float:\n",
    "        contexts.pop(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenize_fn,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"KorSTS/sts-train.tsv\",\n",
    "        k1=1.5, b=0.75, epsilon=0.25,\n",
    "        is_bm25 = True\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        df = pd.read_csv(data_path + context_path, sep='\\t')\n",
    "\n",
    "        self.contexts1 = list(\n",
    "            df.iloc[i].sentence1 for i in range(len(df))\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "        self.contexts2 = list(df.iloc[j].sentence2 for j in range(len(df)))\n",
    "        self.contexts = self.contexts1 + self.contexts2\n",
    "        \n",
    "        for idx, i in enumerate(self.contexts):\n",
    "            if type(i) == float:\n",
    "                self.contexts.pop(idx)\n",
    "        \n",
    "\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "\n",
    "        # Transform by vectorizer\n",
    "        self.tfidfv = TfidfVectorizer(\n",
    "            tokenizer=tokenize_fn, ngram_range=(1, 2), max_features=50000,\n",
    "        )\n",
    "        self.tokenize_fn = tokenize_fn\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "        self.bm25 = None\n",
    "        self.is_bm25 = is_bm25\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.epsilon = epsilon\n",
    "        self.get_sparse_embedding() \n",
    "\n",
    "    def get_sparse_embedding(self) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            Passage Embedding을 만들고\n",
    "            TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "            만약 미리 저장된 파일이 있으면 저장된 pickle을 불러옵니다.\n",
    "        \"\"\"\n",
    "        if not self.is_bm25: # tfidf를 사용하는 경우\n",
    "        # Pickle을 저장합니다.\n",
    "            pickle_name = f\"sparse_embedding.bin\"\n",
    "            tfidfv_name = f\"tfidv.bin\"\n",
    "            emd_path = os.path.join(self.data_path, pickle_name)\n",
    "            tfidfv_path = os.path.join(self.data_path, tfidfv_name)\n",
    "\n",
    "            if os.path.isfile(emd_path) and os.path.isfile(tfidfv_path):\n",
    "                with open(emd_path, \"rb\") as file:\n",
    "                    self.p_embedding = pickle.load(file)\n",
    "                with open(tfidfv_path, \"rb\") as file:\n",
    "                    self.tfidfv = pickle.load(file)\n",
    "                print(\"Embedding pickle load.\")\n",
    "            else:\n",
    "                print(\"Build passage embedding\")\n",
    "                self.p_embedding = self.tfidfv.fit_transform(self.contexts)\n",
    "                print(self.p_embedding.shape)\n",
    "                with open(emd_path, \"wb\") as file:\n",
    "                    pickle.dump(self.p_embedding, file)\n",
    "                with open(tfidfv_path, \"wb\") as file:\n",
    "                    pickle.dump(self.tfidfv, file)\n",
    "                print(\"Embedding pickle saved.\")\n",
    "        \n",
    "        else: # bm25\n",
    "            bm25_name = f\"bm25.bin\"\n",
    "            bm25_path = os.path.join(self.data_path, bm25_name)\n",
    "            if os.path.isfile(bm25_path):\n",
    "                with open(bm25_path, \"rb\") as file:\n",
    "                    self.bm25 = pickle.load(file)\n",
    "                print(\"Embedding bm25 pickle load.\")\n",
    "            else:\n",
    "                print(\"Building bm25... It may take 1 minute and 30 seconds...\")\n",
    "                # bm25 must tokenizer first \n",
    "                # because it runs pool inside and this cuases unexpected result.\n",
    "                tokenized_corpus = []\n",
    "                for c in tqdm(self.contexts):\n",
    "                    tokenized_corpus.append(self.tokenize_fn(c))\n",
    "                self.bm25 = MyBm25(tokenized_corpus, k1 = self.k1, b = self.b, epsilon=self.epsilon)\n",
    "                # bm25 클래스를 불러와서 실행합니다.\n",
    "                \n",
    "                with open(bm25_path, \"wb\") as file:\n",
    "                    pickle.dump(self.bm25, file)\n",
    "                print(\"bm25 pickle saved.\")\n",
    "    \n",
    "    def retrieve(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.p_embedding is not None, \"get_sparse_embedding() 메소드를 먼저 수행해줘야합니다.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc(query_or_dataset, k=topk)\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            total = []\n",
    "            with timer(\"query exhaustive search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk(\n",
    "                    query_or_dataset[\"question\"], k=topk\n",
    "                )\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context_id\": doc_indices[idx],\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            cqas = pd.DataFrame(total)\n",
    "            return cqas\n",
    "    \n",
    "    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        with timer(\"transform\"):\n",
    "            query_vec = self.tfidfv.transform([query])\n",
    "        assert (\n",
    "            np.sum(query_vec) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        with timer(\"query ex search\"):\n",
    "            result = query_vec * self.p_embedding.T\n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = result.toarray()\n",
    "\n",
    "        sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "        doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "        return doc_score, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", use_fast=False,)\n",
    "\n",
    "retriever = SparseRetrieval(\n",
    "    tokenize_fn=tokenizer.tokenize,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_dataset = load_from_disk(dataset_name)\n",
    "full_ds = concatenate_datasets(\n",
    "    [\n",
    "        org_dataset[\"train\"].flatten_indices(),\n",
    "        org_dataset[\"validation\"].flatten_indices(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"bulk query by exhaustive search\"):\n",
    "    df = retriever.retrieve(full_ds)\n",
    "    df[\"correct\"] = df[\"original_context\"] == df[\"context\"]\n",
    "    print(\n",
    "        \"correct retrieval result by exhaustive search\",\n",
    "        df[\"correct\"].sum() / len(df),\n",
    "        )\n",
    "\n",
    "with timer(\"single query by exhaustive search\"):\n",
    "            scores, indices = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BM25:\n",
    "    \"\"\"\n",
    "    Best Match 25.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k1 : float, default 1.5\n",
    "\n",
    "    b : float, default 0.75\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tf_ : list[dict[str, int]]\n",
    "        Term Frequency per document. So [{'hi': 1}] means\n",
    "        the first document contains the term 'hi' 1 time.\n",
    "\n",
    "    df_ : dict[str, int]\n",
    "        Document Frequency per term. i.e. Number of documents in the\n",
    "        corpus that contains the term.\n",
    "\n",
    "    idf_ : dict[str, float]\n",
    "        Inverse Document Frequency per term.\n",
    "\n",
    "    doc_len_ : list[int]\n",
    "        Number of terms per document. So [3] means the first\n",
    "        document contains 3 terms.\n",
    "\n",
    "    corpus_ : list[list[str]]\n",
    "        The input corpus.\n",
    "\n",
    "    corpus_size_ : int\n",
    "        Number of documents in the corpus.\n",
    "\n",
    "    avg_doc_len_ : float\n",
    "        Average number of terms for documents in the corpus.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k1=1.5, b=0.75):\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        \"\"\"\n",
    "        Fit the various statistics that are required to calculate BM25 ranking\n",
    "        score using the corpus given.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : list[list[str]]\n",
    "            Each element in the list represents a document, and each document\n",
    "            is a list of the terms.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        tf = []\n",
    "        df = {}\n",
    "        idf = {}\n",
    "        doc_len = []\n",
    "        corpus_size = 0\n",
    "        for document in corpus:\n",
    "            corpus_size += 1\n",
    "            doc_len.append(len(document))\n",
    "\n",
    "            # compute tf (term frequency) per document\n",
    "            frequencies = {}\n",
    "            for term in document:\n",
    "                term_count = frequencies.get(term, 0) + 1\n",
    "                frequencies[term] = term_count\n",
    "\n",
    "            tf.append(frequencies)\n",
    "\n",
    "            # compute df (document frequency) per term\n",
    "            for term, _ in frequencies.items():\n",
    "                df_count = df.get(term, 0) + 1\n",
    "                df[term] = df_count\n",
    "\n",
    "        for term, freq in df.items():\n",
    "            idf[term] = math.log(1 + (corpus_size - freq + 0.5) / (freq + 0.5))\n",
    "\n",
    "        self.tf_ = tf\n",
    "        self.df_ = df\n",
    "        self.idf_ = idf\n",
    "        self.doc_len_ = doc_len\n",
    "        self.corpus_ = corpus\n",
    "        self.corpus_size_ = corpus_size\n",
    "        self.avg_doc_len_ = sum(doc_len) / corpus_size\n",
    "        return self\n",
    "\n",
    "    def search(self, query):\n",
    "        scores = [self._score(query, index) for index in range(self.corpus_size_)]\n",
    "        return scores\n",
    "\n",
    "    def keyword_search(self, query, index, threshold=0.5):\n",
    "        scores = [self.keyword_score(query, index)]\n",
    "        return scores\n",
    "\n",
    "    def _score(self, query, index):\n",
    "        score = 0.0\n",
    "\n",
    "        doc_len = self.doc_len_[index]\n",
    "        frequencies = self.tf_[index]\n",
    "        for term in query:\n",
    "            if term not in frequencies:\n",
    "                continue\n",
    "\n",
    "            freq = frequencies[term]\n",
    "            numerator = self.idf_[term] * freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len_)\n",
    "            score += (numerator / denominator)\n",
    "\n",
    "        return score\n",
    "    def keyword_score(self, query, index):\n",
    "        \n",
    "        score = []\n",
    "\n",
    "        doc_len = self.doc_len_[index]\n",
    "        frequencies = self.tf_[index]\n",
    "        for term in query:\n",
    "            if term not in frequencies:\n",
    "                continue\n",
    "\n",
    "            freq = frequencies[term]\n",
    "            numerator = self.idf_[term] * freq * (self.k1 + 1)\n",
    "            denominator = freq + self.k1 * (1 - self.b + self.b * doc_len / self.avg_doc_len_)\n",
    "            score.append((numerator / denominator))\n",
    "\n",
    "        return score"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
