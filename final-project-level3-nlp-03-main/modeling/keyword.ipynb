{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec을 통한 임베딩 진행 \n",
    "\n",
    "https://doitgrow.com/27\n",
    "\n",
    "python3 -m pip install konlpy\n",
    "\n",
    "pip install beautifulsoup4\n",
    "\n",
    "pip install --upgrade gensim\n",
    "\n",
    "pip install soynlp\n",
    "\n",
    "apt-get update\n",
    "\n",
    "apt install default-jdk\n",
    "\n",
    "pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load및 설치해야 하는 대상이 상당히 많습니다.\n",
    "\n",
    "위에 친구들을 모두 설치해주어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 객체 import\n",
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from preprocessing import preprocess_data, tokenizing_data, get_label\n",
    "from utils import seed_fix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "# 띄어쓰기\n",
    "from pykospacing import Spacing\n",
    "# 마춤뻡 검사기\n",
    "from hanspell import spell_checker\n",
    "\n",
    "from soynlp.normalizer import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 아래 코드는 직접 word2vec을 만들어줄 때만 필요한 contexts를 만들어주는 파일입니다.\n",
    "\n",
    "이 부분은 넘어가셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"../data/\" + \"KorSTS/sts-train.tsv\", sep='\\t')\n",
    "df2 = pd.read_csv(\"../data/\" + \"KorSTS/sts-dev.tsv\", sep='\\t')\n",
    "df3 = pd.read_csv(\"../data/\" + \"KorSTS/sts-test.tsv\", sep='\\t')\n",
    "df4 = pd.read_csv(\"../data/\" + \"whole_answers.csv\", sep=',', encoding = 'utf-8')\n",
    "data = open(\"../data/para_kqc_sim_data.txt\", 'r', encoding='utf-8')\n",
    "lines = data.readlines()\n",
    "random.shuffle(lines)\n",
    "train_data, test_data = preprocess_data(lines)\n",
    "\n",
    "seed_fix(42)\n",
    "\n",
    "\n",
    "train_contexts1 = list(\n",
    "    df1.iloc[i].sentence1 for i in range(len(df1))\n",
    "    )  # set 은 매번 순서가 바뀌므로\n",
    "train_contexts2 = list(df1.iloc[j].sentence2 for j in range(len(df1)))\n",
    "dev_contexts1 = list(\n",
    "    df2.iloc[i].sentence1 for i in range(len(df2))\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "dev_contexts2 = list(df2.iloc[j].sentence2 for j in range(len(df2)))\n",
    "test_contexts1 = list(\n",
    "    df3.iloc[i].sentence1 for i in range(len(df3))\n",
    "    )\n",
    "test_contexts2 = list(df3.iloc[j].sentence2 for j in range(len(df3)))\n",
    "\n",
    "parpara_contexts1 = list(\n",
    "    train_data.iloc[i].sent_a for i in range(len(train_data))\n",
    "    )\n",
    "parpara_contexts2 = list(train_data.iloc[j].sent_b for j in range(len(train_data)))\n",
    "\n",
    "parpara_contexts3 = list(\n",
    "    test_data.iloc[i].sent_a for i in range(len(test_data))\n",
    "    )\n",
    "parpara_contexts4 = list(test_data.iloc[j].sent_b for j in range(len(test_data)))\n",
    "wai_contexts = list(\n",
    "    df4[\"answeres\"].iloc[i] for i in range(len(df4))\n",
    "    )\n",
    "\n",
    "\n",
    "contexts = train_contexts1 + train_contexts2 + dev_contexts1 + dev_contexts2 + test_contexts1 + test_contexts2 + parpara_contexts1 + parpara_contexts2 + parpara_contexts3 + parpara_contexts4 + wai_contexts\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"contexts.pkl\", \"wb\") as f:\n",
    "    pickle.dump(contexts, f)\n",
    "\n",
    "with open('contexts.pkl','rb') as f:\n",
    "    contexts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 먼저 Okt를 불러와서 형태소 분석을 해줄 tokenize 함수를 선언해줍니다.\n",
    "\n",
    "해당 함수는 단어에 대해서 Noun, Eomi(어미), Josa(조사), Verb 등 태깅을 해줌과 동시에 형태소 분할도 진행해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['비행기', '가', '이륙', '하고', '있다'], ['하다', '남자', '가', '크다', '플루트', '를', '연주', '하고', '있다'], ['하다', '남자', '가', '피자', '에', '치즈', '를', '뿌리다', '있다'], ['세', '남자', '가', '체스', '를', '하다', '있다'], ['하다', '남자', '가', '첼로', '를', '연주', '하고', '있다'], ['몇몇', '남자', '들', '이', '싸우다', '있다'], ['남자', '가', '담배', '를', '피우다', '있다'], ['남자', '가', '피아노', '를', '치고', '있다'], ['하다', '남자', '가', '기타', '를', '치고', '노래', '를', '부르다', '있다'], ['사람', '이', '고양이', '를', '천장', '에', '던지다', '있다']]\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = Okt()\n",
    "\n",
    "def tokenize(doc):\n",
    "    # norm, stem은 optional\n",
    "    if type(doc) is not str:\n",
    "        return []\n",
    "    # return ['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "    return [t[0] for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "tokenized_list = []\n",
    "\n",
    "for i in range(len(contexts)):    \n",
    "    review = tokenize(contexts[i])\n",
    "    tokenized_list.append(review) # 토큰화 된 리뷰를 리스트에 담아줌\n",
    "    \n",
    "print(tokenized_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 방식을 통해서 직접 Word2vec을 구현해줄 수 있습니다.\n",
    "\n",
    "하지만 OOV가 많이 등장하고 성능이 높지 못해 프로토타입으로 남겨두었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이퍼 파라메터 값 지정\n",
    "num_features = 400 # 문자 벡터 차원수\n",
    "min_word_count = 5 # 최소 문자 수\n",
    "num_workers = 4 # 병렬 쓰레드 수\n",
    "context = 10 # window size\n",
    "downsampling = 1e-3 # 문자 빈도 수 downsample\n",
    "\n",
    "#모델 학습\n",
    "model = word2vec.Word2Vec(\n",
    "                            tokenized_list,\n",
    "                            # double_contexts,\n",
    "                            workers = num_workers,\n",
    "                            vector_size = num_features,\n",
    "                            min_count = min_word_count,\n",
    "                            window = context,\n",
    "                            sample = downsampling\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31436/3004066225.py:2: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "# 학습이 완료 되면 필요없는 메모리를 unload 시킨다.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = 'Word2Vec_Kor_POS_Tagging_WAI'\n",
    "# model_name = '300features_50minwords_20text'\n",
    "model.save(model_name)\n",
    "model.wv.save_word2vec_format('my_pos_tagging_wai.embedding', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여기서부터 진짜\n",
    "\n",
    "가장 먼저 유사도를 구해줄 cosine similarity 함수를 선언해줍니다.\n",
    "\n",
    "행렬이 아닌 벡터이기 때문에 sklearn 활용이 불가능해 직접 만들어주었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "\n",
    "    #return np.inner(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 클래스를 통해서 유사도를 계산해줍니다.\n",
    "\n",
    "3중 for문을 돌아야 하는 것을 함수로 넘가도록 구현했습니다.\n",
    "\n",
    "1. 키워드 리스트(키워드 여러 개) + 문장 리스트(답안 여러개)\n",
    "2. 키워드 하나 + 문장 리스트\n",
    "3. 키워드 하나 + 문장 하나\n",
    "\n",
    "이런 식으로 해서 차례대로 값을 반환해주는 방식입니다.\n",
    "\n",
    "현재는 이 때문에 3중 리스트형식으로 구현이 되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Keyword_similarity:\n",
    "    \n",
    "    def __init__(self, model, threshold = 0.5, pos_tagger=None, lemmatizer=None):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.pos_tagger = pos_tagger\n",
    "        self.lemmatizer = lemmatizer\n",
    "    \n",
    "    ### 키워드랑 문장이 리스트로 들어온다.\n",
    "\n",
    "    def keyword_score(self, keyword_list, sentence_list):\n",
    "        idx_list = []\n",
    "        for keyword in keyword_list:\n",
    "            idx_list.append(self.get_keyword_score_list(keyword, sentence_list))\n",
    "\n",
    "        return idx_list\n",
    "\n",
    "    ## 키워드 하나에 대해서 답안들에 대해 조사한다.\n",
    "    def get_keyword_score_list(self, keyword, sentence_list):\n",
    "        idx_list = []\n",
    "        for sentence in sentence_list:\n",
    "            idx_list.append(self.keyword_one_sentence(keyword, sentence))\n",
    "        \n",
    "        return idx_list\n",
    "\n",
    "    ## 하나의 키워드에 대해서 그 문장의 것들에 대한 점수를 리스트로 반환한다.\n",
    "    def keyword_one_sentence(self, keyword, sentence):\n",
    "        # pos_keyword = self.tokenize(keyword)\n",
    "        # keyword_vec = self.model.wv.get_vector(pos_keyword[0])\n",
    "        pos_keyword = pos_tagger.pos(keyword)\n",
    "        keyword_vec = self.model.wv.get_vector(pos_keyword[0][0])\n",
    "        \n",
    "        cosine_list = []\n",
    "        word_list = []\n",
    "        split_sentence = sentence.split()\n",
    "        for word in split_sentence:\n",
    "            \n",
    "            pos_word = pos_tagger.pos(word)            \n",
    "            # pos_word = word\n",
    "            for split_word in pos_word:             \n",
    "                if split_word[1] in ['Noun', 'Verb', 'Adjective', 'Adverb']:\n",
    "                # if split_word[1] in ['Noun']:\n",
    "                    try: \n",
    "                        word_vec = self.model.wv.get_vector(split_word[0])\n",
    "                        # word_vec = self.model.wv.get_vector(pos_word)\n",
    "                        # word_vec = self.model.wv.get_vector(pos_word)\n",
    "                    except:\n",
    "                        continue\n",
    "                    if cosine_similarity(keyword_vec, word_vec) > self.threshold and pos_word not in word_list:\n",
    "                        results = re.finditer(word, sentence)\n",
    "                        for matched_key in results:\n",
    "                            cosine_list.append([matched_key.start(), matched_key.end()])\n",
    "                        \n",
    "                        word_list.append(pos_word[0][0])\n",
    "            \n",
    "        return cosine_list, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keyword_similarity:\n",
    "    \n",
    "    def __init__(self, model, threshold = 0.5, pos_tagger=None, lemmatizer=None):\n",
    "        self.model = model\n",
    "        self.threshold = threshold\n",
    "        self.pos_tagger = pos_tagger\n",
    "        self.lemmatizer = lemmatizer\n",
    "    \n",
    "    ### 키워드랑 문장이 리스트로 들어온다.\n",
    "    def tokenize(self, doc):\n",
    "    # norm, stem은 optional\n",
    "        if type(doc) is not str:\n",
    "            return []\n",
    "        return ['/'.join(t) for t in self.pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "    \n",
    "    def get_keyword_score(self, keyword_list, sentence_list):\n",
    "        results = self.keywords_sentences(keyword_list, sentence_list)\n",
    "        \n",
    "        keyword_num = len(results.keys())\n",
    "        sentence_num = len(results.values())\n",
    "        \n",
    "        keyword_score = []\n",
    "        for keyword, students in results.items():\n",
    "            for idx, sentence in students.items():    \n",
    "                keyword_score.append(len(sentence[0]))\n",
    "        return results, keyword_score\n",
    "        \n",
    "\n",
    "    def keywords_sentences(self, keyword_list, sentence_list):\n",
    "        keyword_dict = {}\n",
    "        for idx, keyword in enumerate(keyword_list):\n",
    "            keyword_dict[f\"keyword_{idx}\"] = self.keyword_sentences(keyword, sentence_list)\n",
    "\n",
    "        return keyword_dict\n",
    "\n",
    "    ## 키워드 하나에 대해서 답안들에 대해 조사한다.\n",
    "    def keyword_sentences(self, keyword, sentence_list):\n",
    "        sentence_dict = {}\n",
    "        for idx, sentence in enumerate(sentence_list):\n",
    "            sentence_dict[f'student_{idx}'] = self.keyword_one_sentence(keyword, sentence)\n",
    "        \n",
    "        return sentence_dict\n",
    "\n",
    "    ## 하나의 키워드에 대해서 그 문장의 것들에 대한 점수를 리스트로 반환한다.\n",
    "    def keyword_one_sentence(self, keyword, sentence):\n",
    "        # pos_keyword = self.tokenize(keyword)\n",
    "        # keyword_vec = self.model.wv.get_vector(pos_keyword[0])\n",
    "        pos_keyword = pos_tagger.pos(keyword)\n",
    "        keyword_vec = self.model.wv.get_vector(pos_keyword[0][0])\n",
    "        \n",
    "        start_idx = []\n",
    "        end_idx = []\n",
    "        word_list = []\n",
    "        split_sentence = sentence.split()\n",
    "        for word in split_sentence:\n",
    "            \n",
    "            pos_word = pos_tagger.pos(word)            \n",
    "            # pos_word = word\n",
    "            for split_word in pos_word:             \n",
    "                if split_word[1] in ['Noun', 'Verb', 'Adjective', 'Adverb']:\n",
    "                # if split_word[1] in ['Noun']:\n",
    "                    try: \n",
    "                        word_vec = self.model.wv.get_vector(split_word[0])\n",
    "                        # word_vec = self.model.wv.get_vector(pos_word)\n",
    "                        # word_vec = self.model.wv.get_vector(pos_word)\n",
    "                    except:\n",
    "                        continue\n",
    "                    if cosine_similarity(keyword_vec, word_vec) > self.threshold and pos_word not in word_list:\n",
    "                        results = re.finditer(word, sentence)\n",
    "                        for matched_key in results:\n",
    "                            start_idx.append(matched_key.start())\n",
    "                            end_idx.append(matched_key.end())\n",
    "                        \n",
    "                        word_list.append(pos_word[0][0])\n",
    "            \n",
    "        return start_idx, end_idx, word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText로 다운 받은 모델을 로드합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import gensim.models as g\n",
    "\n",
    "model_name = 'fast_text_ko'\n",
    "model = g.Doc2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 방식은 gz 파일 자체로 로드하는 방식이어서 9분 정도 걸립니다(비추)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim import models\n",
    "\n",
    "# ko_model = models.fasttext.load_facebook_model('cc.ko.300.bin.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 저장 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ko_model.save(\"fast_text_ko\")\n",
    "# ko_model.wv.save_word2vec_format('fast_text.embedding', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석기와 word2vec load 후 단일 문장에서 제대로 작동하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([9], [11], ['값'])\n",
      "([6, 9], [8, 11], ['맛', '좋은'])\n",
      "([0, 12], [3, 15], ['기압', '부피'])\n"
     ]
    }
   ],
   "source": [
    "pos_tagger = Okt()\n",
    "Fast_KS = Keyword_similarity(model, 0.35, pos_tagger)\n",
    "print(Fast_KS.keyword_one_sentence(\"가격\", \"어느 곳에 빵의 값이 낮은 지 알 수 있다.\"))\n",
    "print(Fast_KS.keyword_one_sentence(\"품질\", \"어느 곳에 맛이 좋은 지 알 수 있다.\"))\n",
    "print(Fast_KS.keyword_one_sentence(\"압력\", \"기압이 낮아지게 되어 부피가 커진다\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 키워드, 여러 문장 가능한지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'keyword_0': {'student_0': ([9, 12], [11, 15], ['값', '저렴한']), 'student_1': ([], [], []), 'student_2': ([6], [9], ['가격'])}, 'keyword_1': {'student_0': ([], [], []), 'student_1': ([6, 9], [8, 11], ['맛', '좋은']), 'student_2': ([6], [9], ['가격'])}}\n"
     ]
    }
   ],
   "source": [
    "print(Fast_KS.keywords_sentences([\"가격\", \"품질\"], [\"어느 곳에 빵의 값이 저렴한 지 알 수 있다.\", \"어느 곳에 맛이 좋은 지 알 수 있다.\", \"어느 곳에 가격이 낮은 지 알 수 있다.\"]))\n",
    "result = Fast_KS.keywords_sentences([\"가격\", \"품질\"], [\"어느 곳에 빵의 값이 저렴한 지 알 수 있다.\", \"어느 곳에 맛이 좋은 지 알 수 있다.\", \"어느 곳에 가격이 낮은 지 알 수 있다.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_values' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B49.50.166.169/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb#ch0000025vscode-remote?line=0'>1</a>\u001b[0m keyword_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(result\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B49.50.166.169/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb#ch0000025vscode-remote?line=1'>2</a>\u001b[0m sentence_num \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(result\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B49.50.166.169/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb#ch0000025vscode-remote?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(result\u001b[39m.\u001b[39;49mvalues()\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B49.50.166.169/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb#ch0000025vscode-remote?line=3'>4</a>\u001b[0m corrected \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B49.50.166.169/opt/ml/project/final-project-level3-nlp-03/modeling/keyword.ipynb#ch0000025vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m keyword, students \u001b[39min\u001b[39;00m result\u001b[39m.\u001b[39mitems():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict_values' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "keyword_num = len(result.keys())\n",
    "sentence_num = len(result.values())\n",
    "corrected = []\n",
    "for keyword, students in result.items():\n",
    "    temp = []\n",
    "    for idx, sentence in students.items():    \n",
    "        if len(sentence[0]) != 0:\n",
    "            temp.append(1)\n",
    "        else : \n",
    "            temp.append(0)\n",
    "\n",
    "    corrected.append(temp)\n",
    "\n",
    "keyword_score = [0 for i in range(sentence_num)]\n",
    "for score in corrected:\n",
    "    for i in range(sentence_num):\n",
    "        keyword_score[i] += score[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(keyword_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
