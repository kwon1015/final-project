{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a777170-40d6-434f-a4bb-1d75565f3a11",
   "metadata": {},
   "source": [
    "# Find Good Answer\n",
    "- 학생의 답안들 중에서 좋은 답을 분류하자"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd1667-b965-4f53-b247-bf70c581dd67",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 01. LDA를 써보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb7b9ac-4d66-416d-bf07-8a792fee049d",
   "metadata": {},
   "source": [
    "- Topic은 2개 = 정답, 오답\n",
    "- LDA 진행 : 한 질문에 대한 모든 답변을 함께 넣어본다\n",
    "- 정답에 해당하는 토픽의 키워드들이 많이 들어있는 답변을 골라보기.\n",
    "- 답변을 출력해서 진짜 정답같은지 확인하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebfd67d-ab30-4b3d-8a55-7e74e3725e56",
   "metadata": {},
   "source": [
    "### Get Data (wai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "70f5ce07-0549-426c-99ce-b649398fd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2567751-df42-45ff-847d-9bcd4dabbea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wai_path = '/opt/ml/StudentScoring/01.data/wai_raw/'\n",
    "wai_questions = os.listdir(wai_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdaed22-5e13-46fd-a0fb-289c58dec32e",
   "metadata": {},
   "source": [
    "### 질문 하나에 대해 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9579fd2d-a2a3-4bed-8927-5ed5eacf475d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'q29.csv'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 일단 하나만 \n",
    "q_29 = wai_questions[0]\n",
    "q_29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7e02532-9a11-4a25-bd76-73d5874c0f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>1) 뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요?</th>\n",
       "      <th>q29_발화점</th>\n",
       "      <th>q29_온도</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>q29_발화점.1</th>\n",
       "      <th>q29_온도.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>공기 땜에</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>발화점 이상의 온도라서</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>발화점 보다 높은 온도</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>다리미가 뜨거워서</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>발화점보다 높은 열을 받아서</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>발화점보다 높은 온도를 받아서</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>전기 때문에</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>발화점 이상의 온도를 받아서</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>1150.0</td>\n",
       "      <td>천이있엇기때문이다</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>1151.0</td>\n",
       "      <td>다리미판이 발화점에 도달하였기 때문이다.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1151</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.4%</td>\n",
       "      <td>73.2%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1154 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0 1) 뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요?  \\\n",
       "0            1.0                                              공기 땜에          \n",
       "1            2.0                 뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.          \n",
       "2            3.0                                          다리미가 뜨거워서          \n",
       "3            4.0                     다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에          \n",
       "4            5.0                                             전기 때문에          \n",
       "...          ...                                                ...          \n",
       "1149      1150.0                                          천이있엇기때문이다          \n",
       "1150      1151.0                             다리미판이 발화점에 도달하였기 때문이다.          \n",
       "1151         NaN                                                NaN          \n",
       "1152         NaN                                                NaN          \n",
       "1153         NaN                                                NaN          \n",
       "\n",
       "     q29_발화점 q29_온도  Unnamed: 4         q29_발화점.1  q29_온도.1  \n",
       "0          0      0         NaN      발화점 이상의 온도라서       NaN  \n",
       "1          0      1         NaN      발화점 보다 높은 온도       NaN  \n",
       "2          0      1         NaN   발화점보다 높은 열을 받아서       NaN  \n",
       "3          0      1         NaN  발화점보다 높은 온도를 받아서       NaN  \n",
       "4          0      0         NaN   발화점 이상의 온도를 받아서       NaN  \n",
       "...      ...    ...         ...               ...       ...  \n",
       "1149       0      0         NaN               NaN       NaN  \n",
       "1150       1      0         NaN               NaN       NaN  \n",
       "1151     NaN    NaN         NaN               NaN       NaN  \n",
       "1152     NaN    NaN         NaN               NaN       NaN  \n",
       "1153   24.4%  73.2%         NaN               NaN       NaN  \n",
       "\n",
       "[1154 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_29_pd = pd.read_csv(os.path.join(wai_path, q_29))\n",
    "q_29_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e148da-7ac0-493a-ad17-4ae1f6612384",
   "metadata": {},
   "source": [
    "#### get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3cd61cf-3af1-4d7e-b567-3fc6f0442692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1) 뜨거운 다리미가 놓여있던 다리미판에 불을 붙이지 않았는데도 다리미판이 타게 된 이유는 무엇일까요?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_29_pd.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "592c21e6-3363-4431-aa8a-6f7200732d5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['공기 땜에',\n",
       " '뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.',\n",
       " '다리미가 뜨거워서',\n",
       " '다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에',\n",
       " '전기 때문에',\n",
       " '다리미의 열이 점점 뜨거워져서 불이 붙는 온도를 넘어서자 천인 다리미판이 탈 물질이 되어 불이 난 것이다.',\n",
       " '뜨거운다리미에 압력에 위하여 불씨가 붙어 불에 탄것이다',\n",
       " '물체가 분해되지 않아 시체가 쌓일것이다.',\n",
       " '다리미가  뜨거워서 천이 탓기 때문',\n",
       " '다리미가 뜨거워 지는 상태에서 천을 모래동안 붙여놓면 불이 날수있다. ',\n",
       " '다리미가 계속 열을 내기때문에 천에 붙어 다리미판이 타게된다.',\n",
       " '\\n뜨거우니까!!!!!!!!!!!!',\n",
       " '다리미 판이 뜨거워서 천이 불에 붙었다.',\n",
       " '다리미의 전력이 불을 불러일으켰기 때문이다',\n",
       " '수증기때문이다',\n",
       " '천에 불이 붙었는데 다리미판위에 있어서 불이 옴겨 붙었기 때문입니다.',\n",
       " '다리미에서 나오는 뜨거운 열이 천에 불이 붙여서 다리미판에 불이 났다',\n",
       " '불이 잘붙는 소질이 아니여서 ',\n",
       " '다리미판이뜨거워서 천이붛에붙었다',\n",
       " '다리미가 전기에 의해서 뜨거워졌기 때문',\n",
       " '다리미가 뜨거워서',\n",
       " '천에서 다리미판으로 불이 전달되어서',\n",
       " '열이 흡수되어서',\n",
       " '열이다식지안아서',\n",
       " '뜨거워서 안에서탐',\n",
       " '다리미 판이 가열되어서 ',\n",
       " '다리미에 열이 점점 천에 전달되서 불이붙었다.',\n",
       " '전기가 다리미를뜨겁게하고 뜨거우니까 탄다',\n",
       " '다리미를 계속 두고 있으면 뜨거워 지기 떄문에',\n",
       " '전기를 이용해서 뜨거워지기 때문에 ',\n",
       " '뜨거운 열에 인해 타게 되었을 것입니다.',\n",
       " '다라미는 뜨거운 열로 옷의 주름을 펴는 도구 이므로 뜨거운 열이 한 곳에 계속 가해지면서 다리미판에 불이 난 것 같다.',\n",
       " '뜨거워서',\n",
       " '뜨거운 다리미가 천으로 된 다리미판의 온도보다 훨씬 높아서 불이붙게 된 것이다. ',\n",
       " '다리미의 온도 때문에 타게된것같다.',\n",
       " ' 다리미에 열이 가해졌기 때문입니다.',\n",
       " '다리미가 전기로 뜨거운 열기를 만들기 때문',\n",
       " '불은 시간이 지날수록 더 뜨거워 지기 떄문이다.\\n',\n",
       " '다리미판이 천이니까',\n",
       " '다리미가 뜨거워 다리미 판이 가열이 됬기 때문이다.',\n",
       " '열이  가열되기 때문',\n",
       " '다리미가 뜨거운 상태로 다리미 바닥 부분을 세워놓지 않고 천이 있는 부분에 놓아  다리미의 뜨거운 열이 천에 불을 붙게 한 것 같다.',\n",
       " '뜨거운 다리미가 놓여있던 다리미판이 가열되면서 불이났기 때문입니다.',\n",
       " '천은 원래 불이 잘 붙으며, 게다가 다리미를 그냥 두고 갔기 때문에 다리미는 전기를 통해서 쓰므로 불이 날 수 있는 경우가 많다.',\n",
       " '발화점 이상의 온도, 탈 물질, 산소가 모두 만족되어었기 때문에',\n",
       " '.',\n",
       " '열',\n",
       " '다리미가 뜨거워서',\n",
       " '뜨거워서',\n",
       " '온도가 높아져서 ',\n",
       " '뜨거운 다리미가 판에 지속적인 열을 주어서',\n",
       " '철이라서',\n",
       " '열로 인해서',\n",
       " '불이 꺼져도 아직 남아있는 열에 의해 탔다.',\n",
       " '물을 타게하는 매개체인 천을 놓았기 때문에 다리미판에 불이 붙은것이다',\n",
       " '한 부분에 온도가 높은 열이 급격히 가해져 순간적인 열로 인해 타게 되었다. ',\n",
       " '발화점 이상의 온도가 되어서',\n",
       " '뜨거워서',\n",
       " '온도가 발화점에 도달했기 때문이다.',\n",
       " '온도가 엄청 높아져서',\n",
       " '\\n다리미가 뜨거워서',\n",
       " '온도가 연소점을 넘어갔기 때문',\n",
       " '온도가 점점 높아져 불이 나게 되는 온도까지 다달했기 때문이다.',\n",
       " '천의 타는점과 다리미판의 타는점이 다르기 때문이다.',\n",
       " '너무 온도가 높아서 타게 되었다.',\n",
       " '온도가 발화점에 도달했기 때문이다.',\n",
       " '불이 붙기 위해서는 불을 붙이기 위한 물체의 발화점 이상에 해당하는 온도를 맞춰주어야 하는데 위에서 설명된 다리미가 천의 발화점이상에 해당하는 온도 였기때문이다. ',\n",
       " '다리미판이 탈 정도로 다리미사 뜨겁기 때문이다.',\n",
       " '다리미판의 온도가 올라 화학반응을 하기 쉬운 환경이 되어 산소와 반응해 연소하게 되었다',\n",
       " '발화점에 도달할만한 온도가 되었기 때문에',\n",
       " '다리미에는 뜨거운 열이 있고 다리미판은 천이기 때문에 천에 열이 가해져서 불이 붙기 때문이다.',\n",
       " '천은 불에 잘타는 물질이기 때문이다',\n",
       " '다리미의 온도가 천의 발화점을 도달해 넘었기 때문이다.',\n",
       " '그러게요',\n",
       " '다리미가 뜨겁기 때문',\n",
       " '온도가 높아서',\n",
       " '다리미는 전기로 열을내서 천을 펴는 것이 목적이기 때문에 열이 가해지면 타면서 불이 붙는다',\n",
       " '다리미의 열이 천을 가열시키기 때문에',\n",
       " '천의 발화점보다 뜨거운 다리미의 온도가 더 높았기 때문이다.',\n",
       " '뜨거운  다리미잖아 열이  높을수록  불이 잘 탄다',\n",
       " '뜨거워서 이다',\n",
       " '지속적으로 다리미판에 뜨거운 다리미가 놓여있어서 쉽게 천이 뜨거워져 불이 난다.',\n",
       " '다리미가 뜨거워서이다.',\n",
       " '온도가 매우 높아서',\n",
       " '다리미판은 불이 붙기위한 발화점이 존재 하는데 뜨거운 다리미의 온도가 발화점에 미치지 못해 판에 불이 붙지는 않지만 발화점에 거의 근접 했으므로 그흔적이 남아 판이 탄 것 이다.',\n",
       " '다리미의 열이 너무 높아 천이 감당할 수 있는 온도가 아니여서 천이 탔다.',\n",
       " '발화점 이상으로 천의 온도가 높아졌기 때문이다.',\n",
       " '연소 과정에서 불은 분자들간의 온도를 높여 진동 속도가 증가할 수 있도록하여 연소가 되게 한다.여기서 다리미는 다리미판의 분자들 간의 온도를 높에 분자의 진동 속도가 증가하여 연소가 되게 하는것이다.',\n",
       " '다리미가 뜨거워서',\n",
       " '다리미에 있던 열이 천을 달궈 열발생',\n",
       " '다리미판이 고열로 가열되어있는 상태에서 산소를 만나 불이 붙었다.',\n",
       " '천이 타서 다리미판으로 옮겨졌기 때문이다',\n",
       " '다리미에 뜨거운 열이 다리미판에도 그 열기가 그대로 옮겨짐으로써 점점 열이 올라서 불이 붙는다고 생각한다.',\n",
       " '천에 열이지속외어 가해졌기 때문이다.',\n",
       " '잘 모르겠다',\n",
       " '뜨거운 다리미에서 다리미판에 많은 열이 전도되었기 때문에.',\n",
       " '다리미판의 열과 천이 반응하여 불이 나게 되었다',\n",
       " '다리미에 열이 있어, 열이 모아모아 점점 뜨거워져 불이 형성되다가 다리미판이 타게 된것이다.',\n",
       " '천은 불에 약하기 때문이다',\n",
       " '열전도율이 높아서 열전도성이 높아졌기 때문이다.']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers_29 = list(q_29_pd[q_29_pd.columns[1]])\n",
    "answers_29[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11803512-18c3-43da-bf2c-be5f21e0d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플 수 :  1154\n"
     ]
    }
   ],
   "source": [
    "print(\"샘플 수 : \", len(answers_29))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05f9c0-d041-4941-a91c-80ad87382d9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### make corpus\n",
    "참고 : https://wikidocs.net/24949"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987806e-f9e3-457e-93f9-cfd3620c0a90",
   "metadata": {},
   "source": [
    "- 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0dfe4d45-38a0-44f0-8d21-c8f1e8eebf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137\n",
      "['공기 땜에', '뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.', '다리미가 뜨거워서', '다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에', '전기 때문에', '다리미의 열이 점점 뜨거워져서 불이 붙는 온도를 넘어서자 천인 다리미판이 탈 물질이 되어 불이 난 것이다.', '뜨거운다리미에 압력에 위하여 불씨가 붙어 불에 탄것이다', '물체가 분해되지 않아 시체가 쌓일것이다.', '다리미가  뜨거워서 천이 탓기 때문', '다리미가 뜨거워 지는 상태에서 천을 모래동안 붙여놓면 불이 날수있다. ', '다리미가 계속 열을 내기때문에 천에 붙어 다리미판이 타게된다.', '\\n뜨거우니까!!!!!!!!!!!!', '다리미 판이 뜨거워서 천이 불에 붙었다.', '다리미의 전력이 불을 불러일으켰기 때문이다', '수증기때문이다', '천에 불이 붙었는데 다리미판위에 있어서 불이 옴겨 붙었기 때문입니다.', '다리미에서 나오는 뜨거운 열이 천에 불이 붙여서 다리미판에 불이 났다', '불이 잘붙는 소질이 아니여서 ', '다리미판이뜨거워서 천이붛에붙었다', '다리미가 전기에 의해서 뜨거워졌기 때문', '다리미가 뜨거워서', '천에서 다리미판으로 불이 전달되어서', '열이 흡수되어서', '열이다식지안아서', '뜨거워서 안에서탐', '다리미 판이 가열되어서 ', '다리미에 열이 점점 천에 전달되서 불이붙었다.', '전기가 다리미를뜨겁게하고 뜨거우니까 탄다', '다리미를 계속 두고 있으면 뜨거워 지기 떄문에', '전기를 이용해서 뜨거워지기 때문에 ', '뜨거운 열에 인해 타게 되었을 것입니다.', '다라미는 뜨거운 열로 옷의 주름을 펴는 도구 이므로 뜨거운 열이 한 곳에 계속 가해지면서 다리미판에 불이 난 것 같다.', '뜨거워서', '뜨거운 다리미가 천으로 된 다리미판의 온도보다 훨씬 높아서 불이붙게 된 것이다. ', '다리미의 온도 때문에 타게된것같다.', ' 다리미에 열이 가해졌기 때문입니다.', '다리미가 전기로 뜨거운 열기를 만들기 때문', '불은 시간이 지날수록 더 뜨거워 지기 떄문이다.\\n', '다리미판이 천이니까', '다리미가 뜨거워 다리미 판이 가열이 됬기 때문이다.', '열이  가열되기 때문', '다리미가 뜨거운 상태로 다리미 바닥 부분을 세워놓지 않고 천이 있는 부분에 놓아  다리미의 뜨거운 열이 천에 불을 붙게 한 것 같다.', '뜨거운 다리미가 놓여있던 다리미판이 가열되면서 불이났기 때문입니다.', '천은 원래 불이 잘 붙으며, 게다가 다리미를 그냥 두고 갔기 때문에 다리미는 전기를 통해서 쓰므로 불이 날 수 있는 경우가 많다.', '발화점 이상의 온도, 탈 물질, 산소가 모두 만족되어었기 때문에', '.', '열', '다리미가 뜨거워서', '뜨거워서', '온도가 높아져서 ']\n",
      "['공기 땜에', '뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.', '다리미가 뜨거워서', '다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에', '전기 때문에', '다리미의 열이 점점 뜨거워져서 불이 붙는 온도를 넘어서자 천인 다리미판이 탈 물질이 되어 불이 난 것이다.', '뜨거운다리미에 압력에 위하여 불씨가 붙어 불에 탄것이다', '물체가 분해되지 않아 시체가 쌓일것이다.', '다리미가  뜨거워서 천이 탓기 때문', '다리미가 뜨거워 지는 상태에서 천을 모래동안 붙여놓면 불이 날수있다. ', '다리미가 계속 열을 내기때문에 천에 붙어 다리미판이 타게된다.', '\\n뜨거우니까', '다리미 판이 뜨거워서 천이 불에 붙었다.', '다리미의 전력이 불을 불러일으켰기 때문이다', '수증기때문이다', '천에 불이 붙었는데 다리미판위에 있어서 불이 옴겨 붙었기 때문입니다.', '다리미에서 나오는 뜨거운 열이 천에 불이 붙여서 다리미판에 불이 났다', '불이 잘붙는 소질이 아니여서 ', '다리미판이뜨거워서 천이붛에붙었다', '다리미가 전기에 의해서 뜨거워졌기 때문', '다리미가 뜨거워서', '천에서 다리미판으로 불이 전달되어서', '열이 흡수되어서', '열이다식지안아서', '뜨거워서 안에서탐', '다리미 판이 가열되어서 ', '다리미에 열이 점점 천에 전달되서 불이붙었다.', '전기가 다리미를뜨겁게하고 뜨거우니까 탄다', '다리미를 계속 두고 있으면 뜨거워 지기 떄문에', '전기를 이용해서 뜨거워지기 때문에 ', '뜨거운 열에 인해 타게 되었을 것입니다.', '다라미는 뜨거운 열로 옷의 주름을 펴는 도구 이므로 뜨거운 열이 한 곳에 계속 가해지면서 다리미판에 불이 난 것 같다.', '뜨거워서', '뜨거운 다리미가 천으로 된 다리미판의 온도보다 훨씬 높아서 불이붙게 된 것이다. ', '다리미의 온도 때문에 타게된것같다.', ' 다리미에 열이 가해졌기 때문입니다.', '다리미가 전기로 뜨거운 열기를 만들기 때문', '불은 시간이 지날수록 더 뜨거워 지기 떄문이다.\\n', '다리미판이 천이니까', '다리미가 뜨거워 다리미 판이 가열이 됬기 때문이다.', '열이  가열되기 때문', '다리미가 뜨거운 상태로 다리미 바닥 부분을 세워놓지 않고 천이 있는 부분에 놓아  다리미의 뜨거운 열이 천에 불을 붙게 한 것 같다.', '뜨거운 다리미가 놓여있던 다리미판이 가열되면서 불이났기 때문입니다.', '천은 원래 불이 잘 붙으며 게다가 다리미를 그냥 두고 갔기 때문에 다리미는 전기를 통해서 쓰므로 불이 날 수 있는 경우가 많다.', '발화점 이상의 온도 탈 물질 산소가 모두 만족되어었기 때문에', '.', '열', '다리미가 뜨거워서', '뜨거워서', '온도가 높아져서 ']\n",
      "전처리 후 샘플 수 :  1138\n"
     ]
    }
   ],
   "source": [
    "# 전처리 \n",
    "import re\n",
    "\n",
    "## 특수 문자 제거\n",
    "answers_edited_29 = []\n",
    "for ans in answers_29:\n",
    "    if ans==ans:\n",
    "        #print(ans)\n",
    "        n_ans = re.sub(r\"[^a-zA-Z가-힣\\s\\t\\.]\", \"\", ans)\n",
    "        if n_ans:\n",
    "            answers_edited_29.append(n_ans)\n",
    "\n",
    "print(len(answers_edited_29))\n",
    "        \n",
    "print(answers_29[:50])\n",
    "print(answers_edited_29[:50])\n",
    "print(\"전처리 후 샘플 수 : \", len(answers_removed_29)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75babae0-b925-40b0-a75b-dbb9e60662b7",
   "metadata": {},
   "source": [
    "- 토크나이징"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412fd413-63ed-48a5-8375-764e89633ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['점점', '더', '시간', '이', '지나', '면서', '점차', '가열', '되', '비', '때문', '에']\n",
      "[('점점', 'MAG'), ('더', 'MAG'), ('시간', 'NNG'), ('이', 'JKS'), ('지나', 'VV'), ('면서', 'EC'), ('점차', 'MAG'), ('가열', 'NNG'), ('되', 'XSV'), ('비', 'XPN'), ('때문', 'NNB'), ('에', 'JKB')]\n",
      "['시간', '가열', '때문']\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer= Mecab()\n",
    "text= '점점더시간이지나면서점차가열되비때문에'\n",
    "print(tokenizer.morphs(text))\n",
    "print(tokenizer.pos(text))  \n",
    "print(tokenizer.nouns(text))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dc7a4f5-48a4-4f22-b04f-e3cffa427e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['공기'], ['다리미', '전달'], ['다리미'], ['다리미판', '불', '정도', '불'], ['전기']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "# docs에서는 띄어쓰기로 하지만, \n",
    "# 띄어쓰기 아예안된 문장도 많아서 우선 Mecab 사용\n",
    "\n",
    "# 1. 띄어쓰기 토큰화\n",
    "#tokenized_ans_29 = [ans.split() for ans in answers_removed_29] # 토큰화\n",
    "#tokenized_ans_29[:10]\n",
    "\n",
    "# 2. Mecab \n",
    "tokenized_ans_29 =[]\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer= Mecab()\n",
    "for ans in answers_removed_29:\n",
    "    pos = tokenizer.pos(ans)\n",
    "    pos_cleaned = [p[0] for p in pos if p[1] in ['NNG', 'NNP', 'VA']] # p[1][0] not in ['J', 'S', 'E']]\n",
    "    tokenized_ans_29.append(pos_cleaned)\n",
    "    \n",
    "    \n",
    "# 3. Mecab tokenizing + subword segmentation(?)\n",
    "\n",
    "\n",
    "# ++ 불용어 제거도 해야할 듯\n",
    "\n",
    "print(sum([len(a) for a in tokenized_ans_29]))\n",
    "tokenized_ans_29[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a22afd-3ba5-4d37-80cc-b790432fb50f",
   "metadata": {},
   "source": [
    "- 단어 집합 만들기\n",
    "\n",
    "참고: https://wikidocs.net/30708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a893e56d-eaff-4667-bb9a-0d0e90ce9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_ans_29)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_ans_29]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bab5a58c-c333-4886-8124-298b625d7a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "천\n"
     ]
    }
   ],
   "source": [
    "print(dictionary[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8991ab30-a450-47e0-8ab5-1448b2e527ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269cdff-a788-47b3-9ce8-e34020213f45",
   "metadata": {},
   "source": [
    "- LDA 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70a550de-3b0a-4be4-8ada-7b9872d4756a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.139*\"불\" + 0.068*\"발화점\" + 0.056*\"온도\" + 0.049*\"이상\" + 0.047*\"산소\" + 0.047*\"물질\" + 0.038*\"다리미판\" + 0.035*\"다리미\" + 0.021*\"조건\" + 0.019*\"있\"')\n",
      "(1, '0.189*\"다리미\" + 0.112*\"온도\" + 0.111*\"열\" + 0.079*\"다리미판\" + 0.055*\"높\" + 0.031*\"발화점\" + 0.024*\"가열\" + 0.020*\"뜨겁\" + 0.020*\"판\" + 0.019*\"이상\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 2# k=2, 정답  & 오답\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5167c-4934-4ec0-91c2-da9191dd9280",
   "metadata": {},
   "source": [
    "오답을 어떻게 분리하지..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdcbcdb8-1ebc-4e91-ae8e-f6f9a96d71cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el108021401693761296485671498946\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el108021401693761296485671498946_data = {\"mdsDat\": {\"x\": [0.14183371486649213, -0.14183371486649213], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [53.06670618178879, 46.93329381821121]}, \"tinfo\": {\"Term\": [\"\\ubd88\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\uc5f4\", \"\\ub192\", \"\\uc0b0\\uc18c\", \"\\ubb3c\\uc9c8\", \"\\uc628\\ub3c4\", \"\\uc870\\uac74\", \"\\uac00\\uc5f4\", \"\\uc804\\ub2ec\", \"\\ub728\\uac81\", \"\\ud310\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\uc788\", \"\\uc804\\ub3c4\", \"\\uc774\\ub3d9\", \"\\uc815\\ub3c4\", \"\\uc774\\uc0c1\", \"\\ubc1c\\ud654\\uc810\", \"\\ucc9c\\uc740\", \"\\ucda9\\uc871\", \"\\uc804\\uae30\", \"\\uc5f4\\uae30\", \"\\uc77c\\uc815\", \"\\uacf5\\uae30\", \"\\uacf5\\uae09\", \"\\uc704\", \"\\ud544\\uc694\", \"\\uc9c0\\uc18d\", \"\\uc5f0\\uc18c\", \"\\uc870\\uac74\", \"\\uc815\\ub3c4\", \"\\uc0b0\\uc18c\", \"\\ubb3c\\uc9c8\", \"\\ucc9c\\uc740\", \"\\ucda9\\uc871\", \"\\uacf5\\uae30\", \"\\ud544\\uc694\", \"\\ub728\\uac70\\uc6b0\", \"\\ubaa8\\ub984\", \"\\uc5f0\\uc18c\", \"\\ub9cc\\uc871\", \"\\uc774\\uc720\", \"\\ubd88\", \"\\uc131\\uc9c8\", \"\\uc804\\uae30\", \"\\uc27d\", \"\\ubb3c\", \"\\uc9c0\\uc18d\", \"\\uacbd\\uc6b0\", \"\\uc0c1\\ud669\", \"\\uacf3\", \"\\uc788\", \"\\uacf5\\uae09\", \"\\ubb3c\\uccb4\", \"\\uc874\\uc7ac\", \"\\uc5d0\\ub108\\uc9c0\", \"\\uc0dd\\uac01\", \"\\uc694\\uc18c\", \"\\ubc18\\uc751\", \"\\uc5f4\\uae30\", \"\\uc704\", \"\\uc774\\uc0c1\", \"\\ubc1c\\ud654\\uc810\", \"\\ub3c4\\ub2ec\", \"\\ub54c\", \"\\uc628\\ub3c4\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ucc9c\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\uc5f4\", \"\\uc804\\ub2ec\", \"\\uc774\\ub3d9\", \"\\ub192\", \"\\uc804\\ub3c4\", \"\\ud0c0\\ub294\\uc810\", \"\\ucca0\", \"\\ub728\\uac81\", \"\\uc77c\\uc815\", \"\\ubc1c\\ud654\", \"\\uc2dc\\uac04\", \"\\ud310\", \"\\ubd84\\uc790\", \"\\uc5f4\", \"\\uc810\", \"\\uac00\\uc5f4\", \"\\uacc4\\uc18d\", \"\\ub300\\ubb38\", \"\\ub2e4\\ub9ac\", \"\\uc5f4\\uc804\\ub3c4\", \"\\uacb0\\uad6d\", \"\\ubc14\\ub2e5\", \"\\uc99d\\uac00\", \"\\uc18c\\uc7ac\", \"\\ub2e4\\ub974\", \"\\ub0ae\", \"\\u3163\", \"\\ucd08\\uacfc\", \"\\uc774\\ub3d9\\ud558\", \"\\uc0c1\\uc2b9\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ubd80\\ubd84\", \"\\uc628\\ub3c4\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\uc0c1\\ud0dc\", \"\\ucc9c\", \"\\uc2dc\\uc791\", \"\\ubc1c\\ud654\\uc810\", \"\\ucc9c\\uc5d0\", \"\\ud2b9\\uc815\", \"\\ubc1c\\uc0dd\", \"\\uac19\", \"\\uc774\\uc0c1\", \"\\uc637\", \"\\ub3c4\\ub2ec\", \"\\ubd88\"], \"Freq\": [327.0, 460.0, 242.0, 114.0, 108.0, 108.0, 351.0, 48.0, 52.0, 36.0, 43.0, 43.0, 245.0, 46.0, 24.0, 21.0, 24.0, 148.0, 216.0, 20.0, 20.0, 23.0, 27.0, 19.0, 19.0, 22.0, 23.0, 17.0, 19.0, 16.0, 47.58406669643003, 23.551782638627785, 106.32723290567587, 106.17471410182331, 19.71411179792625, 19.707925519629576, 18.653917226834086, 16.782651497688786, 14.86731909967951, 13.000625469657844, 15.777616851675315, 13.897272421081654, 12.013582371743873, 314.924903053586, 10.108740025559385, 22.023541604153138, 10.063262006318979, 15.501352410289606, 18.192720460937196, 8.18481962389355, 8.156728891962732, 9.931300746123055, 43.28805559538389, 20.73036224255239, 10.791568451354486, 13.47827460887994, 10.773872695250049, 8.067978088445088, 6.260564343433305, 9.826572421072273, 25.74986366111447, 22.139779407011684, 111.58293268349287, 153.90718386777465, 39.13992358946781, 13.758392797401283, 127.7554191717729, 86.40554062401672, 33.81973481832794, 80.04402669826952, 19.793373488602384, 35.5049454692081, 21.019540036891623, 111.29573084909093, 23.13757842356975, 9.008953034890292, 7.292003965130761, 40.4117838229226, 17.71945263262583, 8.017800605956076, 11.191191984848183, 39.91521351110659, 5.580203366650474, 222.2577439655977, 9.505088896267166, 48.271827532861955, 5.53580299892199, 4.711824763211483, 4.684204926906797, 3.8597695839331796, 3.848937987043954, 3.8320409014432055, 3.808981860447407, 3.7862657082928175, 3.774121417065993, 2.9983980120660068, 2.9921080572942653, 2.9879905882936986, 2.955994481329005, 7.350877605445534, 380.1404643039042, 10.460460360755114, 224.19337856898093, 158.67994156562744, 10.086267730378003, 33.52499878963725, 6.505884581558208, 62.49955807397303, 12.37788354917146, 4.863146654067909, 10.332562822984167, 11.47330821688746, 37.40208309217396, 8.313733296261834, 14.17462107710648, 12.921647324603672], \"Total\": [327.0, 460.0, 242.0, 114.0, 108.0, 108.0, 351.0, 48.0, 52.0, 36.0, 43.0, 43.0, 245.0, 46.0, 24.0, 21.0, 24.0, 148.0, 216.0, 20.0, 20.0, 23.0, 27.0, 19.0, 19.0, 22.0, 23.0, 17.0, 19.0, 16.0, 48.22608085473363, 24.086358196029447, 108.85768878768525, 108.84090654912396, 20.224887296042787, 20.224262917669048, 19.24841599608184, 17.322928574771446, 15.392444951272875, 13.467478583119759, 16.352540825734025, 14.426006780650386, 12.4990919199034, 327.84655037818965, 10.569870042803037, 23.05831402505393, 10.564847476194801, 16.3219411150606, 19.197499217676896, 8.638612392195318, 8.63551096923288, 10.550303746409366, 46.03348222618687, 22.056188428612362, 11.504741679198528, 14.37983634999958, 11.502828036022148, 8.625618701411803, 6.707255306902138, 10.538791609766507, 27.767290566929816, 23.930858685127994, 148.98501577566682, 216.40674194174767, 53.31454466657429, 16.13005681033169, 351.94879774075383, 245.08548218964415, 67.3447336079652, 460.18449100217373, 242.0511174542001, 36.18428836928642, 21.552291527393376, 114.72495308781455, 24.188493973366896, 9.512467335380375, 7.792655949868652, 43.306104533080386, 19.06204351880129, 8.668854633081885, 12.140724498213675, 43.36753393544442, 6.072192376345557, 242.0511174542001, 10.417076822209744, 52.961075076140084, 6.077621867584793, 5.213466759795338, 5.216859558735165, 4.352770822886356, 4.354115523281257, 4.35620300176342, 4.3589934080858725, 4.361779939955326, 4.363288430050467, 3.4932307844885924, 3.4939851688991936, 3.4944952930338347, 3.498416389319236, 8.751214351806496, 460.18449100217373, 13.197206072644786, 351.94879774075383, 245.08548218964415, 14.209548454909157, 67.3447336079652, 8.855793522308996, 216.40674194174767, 23.58719903514855, 6.1606671593461515, 19.009635787655885, 22.73299179762328, 148.98501577566682, 15.394920766388733, 53.31454466657429, 327.84655037818965], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.8644, -4.5677, -3.0604, -3.0618, -4.7456, -4.7459, -4.8009, -4.9066, -5.0278, -5.1619, -4.9683, -5.0952, -5.2409, -1.9746, -5.4135, -4.6348, -5.418, -4.986, -4.8259, -5.6246, -5.6281, -5.4312, -3.959, -4.6953, -5.3482, -5.1258, -5.3498, -5.639, -5.8926, -5.4418, -4.4785, -4.6295, -3.0122, -2.6906, -4.0598, -5.1053, -2.8768, -3.2679, -4.2059, -3.3443, -4.7416, -4.0344, -4.5586, -2.8919, -4.4626, -5.4059, -5.6173, -3.905, -4.7294, -5.5224, -5.189, -3.9173, -5.8849, -2.2003, -5.3523, -3.7272, -5.8929, -6.054, -6.0599, -6.2535, -6.2563, -6.2607, -6.2667, -6.2727, -6.2759, -6.506, -6.5081, -6.5095, -6.5203, -5.6093, -1.6636, -5.2565, -2.1916, -2.5372, -5.2929, -4.0918, -5.7314, -3.4689, -5.0882, -6.0224, -5.2688, -5.1641, -3.9824, -5.4862, -4.9526, -5.0452], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.6202, 0.6112, 0.6101, 0.6088, 0.608, 0.6078, 0.6022, 0.6019, 0.5989, 0.5983, 0.5978, 0.5963, 0.594, 0.5934, 0.589, 0.5877, 0.585, 0.582, 0.5799, 0.5797, 0.5766, 0.5732, 0.5721, 0.5716, 0.5696, 0.5689, 0.5682, 0.5668, 0.5647, 0.5636, 0.5582, 0.5558, 0.3445, 0.2928, 0.3246, 0.4746, -0.3797, -0.4089, -0.0552, -1.1154, -1.8702, 0.7375, 0.7314, 0.7261, 0.712, 0.7021, 0.69, 0.6873, 0.6834, 0.6784, 0.675, 0.6735, 0.6719, 0.6711, 0.6648, 0.6637, 0.6631, 0.6553, 0.6487, 0.6362, 0.6331, 0.6282, 0.6216, 0.6149, 0.6114, 0.6037, 0.6014, 0.5999, 0.588, 0.5821, 0.5654, 0.524, 0.3055, 0.3217, 0.4137, 0.0589, 0.4481, -0.4856, 0.1117, 0.5199, 0.1468, 0.0726, -0.6257, 0.1403, -0.5683, -2.4772]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.28620613759361135, 0.8586184127808341, 0.0944089596521916, 0.9063260126610395, 0.48387823731806573, 0.48387823731806573, 0.2296677694133391, 0.9186710776533564, 0.9260746560671849, 0.16453804165302463, 0.9872282499181477, 0.9478400091943653, 0.09478400091943653, 0.9521137375104112, 0.04533874940525768, 0.9870942109661176, 0.05195232689295356, 0.8588038366435039, 0.026149498598650053, 0.967531448150052, 0.2291849406775141, 0.9167397627100564, 0.1916862029236707, 0.9584310146183534, 0.1738433205903545, 0.825755772804184, 0.35089797743896656, 0.6487532373580894, 0.1918109477002317, 0.9590547385011585, 0.7315077010204902, 0.26259250805863754, 0.8679448661974124, 0.12399212374248748, 0.9745040536110268, 0.06496693690740178, 0.06927429821604895, 0.9236573095473194, 0.9704695286001259, 0.06931925204286614, 0.965288336622588, 0.9802755620308213, 0.06126722262692633, 0.9738985401794522, 0.02756316623149393, 0.9561275087026821, 0.08692068260933473, 0.2295577133561483, 0.9182308534245932, 0.9488753901095076, 0.09488753901095076, 0.47344410490201233, 0.5260490054466803, 0.11535549300640281, 0.9228439440512225, 0.7116229310519988, 0.28649754367028524, 0.22732084226663782, 0.7577361408887927, 0.9881109866303339, 0.9608153559542706, 0.039652697229858785, 0.9737483973845995, 0.02755891690711131, 0.11426985556508189, 0.7998889889555733, 0.28150085223982385, 0.7037521305995597, 0.9264072535490815, 0.9274697012390074, 0.11593371265487593, 0.9460854257909198, 0.22926420263426725, 0.917056810537069, 0.9465351982158245, 0.09465351982158246, 0.08236740732788517, 0.9060414806067368, 0.22584085717013583, 0.7904430000954753, 0.9562865727934473, 0.08693514298122249, 0.9784412202671752, 0.06115257626669845, 0.08262717483130115, 0.9171616406274428, 0.9363535105209503, 0.07202719311699617, 0.9189548824781841, 0.363689266227541, 0.6364562158981968, 0.4546954223553322, 0.5196519112632368, 0.8945536923017775, 0.9193151106471603, 0.08357410096792367, 0.04639877846534234, 0.9743743477721892, 0.28584361857354323, 0.8575308557206296, 0.751753452633406, 0.24834712274496448, 0.9600697456181875, 0.052460272636230175, 0.9442849074521431, 0.9341026991770519, 0.0651699557565385, 0.954102714365672, 0.043368305198439636, 0.027636304182475226, 0.9949069505691082, 0.041341970322793346, 0.9508653174242471, 0.09599622015534613, 0.9599622015534613, 0.9964146428726746, 0.04151727678636144, 0.9953120624623296, 0.0207356679679652, 0.9040436680630499, 0.0695418206202346, 0.2294107621601386, 0.9176430486405543, 0.9376221244184634, 0.052090118023247964, 0.5048650158440696, 0.5048650158440696, 0.4663546520978735, 0.5087505295613165, 0.9888806650563244, 0.049444033252816225, 0.12832595285011336, 0.8982816699507935, 0.28616435740905655, 0.8584930722271696, 0.9889111945101782, 0.04944555972550891, 0.10512519672794367, 0.946126770551493, 0.16232008224676314, 0.8116004112338158, 0.0691761723058938, 0.922348964078584, 0.9813583151730044, 0.05772695971605908], \"Term\": [\"\\u3163\", \"\\u3163\", \"\\uac00\\uc5f4\", \"\\uac00\\uc5f4\", \"\\uac19\", \"\\uac19\", \"\\uacb0\\uad6d\", \"\\uacb0\\uad6d\", \"\\uacbd\\uc6b0\", \"\\uacc4\\uc18d\", \"\\uacc4\\uc18d\", \"\\uacf3\", \"\\uacf3\", \"\\uacf5\\uae09\", \"\\uacf5\\uae09\", \"\\uacf5\\uae30\", \"\\uacf5\\uae30\", \"\\ub0ae\", \"\\ub192\", \"\\ub192\", \"\\ub2e4\\ub974\", \"\\ub2e4\\ub974\", \"\\ub2e4\\ub9ac\", \"\\ub2e4\\ub9ac\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ub300\\ubb38\", \"\\ub300\\ubb38\", \"\\ub3c4\\ub2ec\", \"\\ub3c4\\ub2ec\", \"\\ub54c\", \"\\ub54c\", \"\\ub728\\uac70\\uc6b0\", \"\\ub728\\uac70\\uc6b0\", \"\\ub728\\uac81\", \"\\ub728\\uac81\", \"\\ub9cc\\uc871\", \"\\ub9cc\\uc871\", \"\\ubaa8\\ub984\", \"\\ubb3c\", \"\\ubb3c\", \"\\ubb3c\\uc9c8\", \"\\ubb3c\\uc9c8\", \"\\ubb3c\\uccb4\", \"\\ubb3c\\uccb4\", \"\\ubc14\\ub2e5\", \"\\ubc14\\ub2e5\", \"\\ubc18\\uc751\", \"\\ubc18\\uc751\", \"\\ubc1c\\uc0dd\", \"\\ubc1c\\uc0dd\", \"\\ubc1c\\ud654\", \"\\ubc1c\\ud654\", \"\\ubc1c\\ud654\\uc810\", \"\\ubc1c\\ud654\\uc810\", \"\\ubd80\\ubd84\", \"\\ubd80\\ubd84\", \"\\ubd84\\uc790\", \"\\ubd88\", \"\\ubd88\", \"\\uc0b0\\uc18c\", \"\\uc0b0\\uc18c\", \"\\uc0c1\\uc2b9\", \"\\uc0c1\\uc2b9\", \"\\uc0c1\\ud0dc\", \"\\uc0c1\\ud0dc\", \"\\uc0c1\\ud669\", \"\\uc0dd\\uac01\", \"\\uc0dd\\uac01\", \"\\uc131\\uc9c8\", \"\\uc18c\\uc7ac\", \"\\uc18c\\uc7ac\", \"\\uc27d\", \"\\uc27d\", \"\\uc2dc\\uac04\", \"\\uc2dc\\uac04\", \"\\uc2dc\\uc791\", \"\\uc2dc\\uc791\", \"\\uc5d0\\ub108\\uc9c0\", \"\\uc5d0\\ub108\\uc9c0\", \"\\uc5f0\\uc18c\", \"\\uc5f0\\uc18c\", \"\\uc5f4\", \"\\uc5f4\", \"\\uc5f4\\uae30\", \"\\uc5f4\\uae30\", \"\\uc5f4\\uc804\\ub3c4\", \"\\uc628\\ub3c4\", \"\\uc628\\ub3c4\", \"\\uc637\", \"\\uc637\", \"\\uc694\\uc18c\", \"\\uc704\", \"\\uc704\", \"\\uc774\\ub3d9\", \"\\uc774\\ub3d9\", \"\\uc774\\ub3d9\\ud558\", \"\\uc774\\ub3d9\\ud558\", \"\\uc774\\uc0c1\", \"\\uc774\\uc0c1\", \"\\uc774\\uc720\", \"\\uc77c\\uc815\", \"\\uc77c\\uc815\", \"\\uc788\", \"\\uc788\", \"\\uc804\\uae30\", \"\\uc804\\uae30\", \"\\uc804\\ub2ec\", \"\\uc804\\ub2ec\", \"\\uc804\\ub3c4\", \"\\uc804\\ub3c4\", \"\\uc810\", \"\\uc810\", \"\\uc815\\ub3c4\", \"\\uc815\\ub3c4\", \"\\uc870\\uac74\", \"\\uc870\\uac74\", \"\\uc874\\uc7ac\", \"\\uc874\\uc7ac\", \"\\uc99d\\uac00\", \"\\uc99d\\uac00\", \"\\uc9c0\\uc18d\", \"\\uc9c0\\uc18d\", \"\\ucc9c\", \"\\ucc9c\", \"\\ucc9c\\uc5d0\", \"\\ucc9c\\uc5d0\", \"\\ucc9c\\uc740\", \"\\ucc9c\\uc740\", \"\\ucca0\", \"\\ucca0\", \"\\ucd08\\uacfc\", \"\\ucd08\\uacfc\", \"\\ucda9\\uc871\", \"\\ucda9\\uc871\", \"\\ud0c0\\ub294\\uc810\", \"\\ud0c0\\ub294\\uc810\", \"\\ud2b9\\uc815\", \"\\ud2b9\\uc815\", \"\\ud310\", \"\\ud310\", \"\\ud544\\uc694\", \"\\ud544\\uc694\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el108021401693761296485671498946\", ldavis_el108021401693761296485671498946_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el108021401693761296485671498946\", ldavis_el108021401693761296485671498946_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el108021401693761296485671498946\", ldavis_el108021401693761296485671498946_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ae405-bb75-44a5-9329-a7bdd70850a3",
   "metadata": {},
   "source": [
    "- 정답에 가까운 답 더 추가(키워드별로 적어놓은 답변)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d1d9038-d83c-4ec6-a751-5aa32b67b2fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_answers = answers_29 + list(q_29_pd[q_29_pd.columns[5]])\n",
    "len(more_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21c2cadd-c77c-410a-8f0d-2412fd16a122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n",
      "['공기 땜에', '뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.', '다리미가 뜨거워서', '다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에', '전기 때문에', '다리미의 열이 점점 뜨거워져서 불이 붙는 온도를 넘어서자 천인 다리미판이 탈 물질이 되어 불이 난 것이다.', '뜨거운다리미에 압력에 위하여 불씨가 붙어 불에 탄것이다', '물체가 분해되지 않아 시체가 쌓일것이다.', '다리미가  뜨거워서 천이 탓기 때문', '다리미가 뜨거워 지는 상태에서 천을 모래동안 붙여놓면 불이 날수있다. ', '다리미가 계속 열을 내기때문에 천에 붙어 다리미판이 타게된다.', '\\n뜨거우니까!!!!!!!!!!!!', '다리미 판이 뜨거워서 천이 불에 붙었다.', '다리미의 전력이 불을 불러일으켰기 때문이다', '수증기때문이다', '천에 불이 붙었는데 다리미판위에 있어서 불이 옴겨 붙었기 때문입니다.', '다리미에서 나오는 뜨거운 열이 천에 불이 붙여서 다리미판에 불이 났다', '불이 잘붙는 소질이 아니여서 ', '다리미판이뜨거워서 천이붛에붙었다', '다리미가 전기에 의해서 뜨거워졌기 때문', '다리미가 뜨거워서', '천에서 다리미판으로 불이 전달되어서', '열이 흡수되어서', '열이다식지안아서', '뜨거워서 안에서탐', '다리미 판이 가열되어서 ', '다리미에 열이 점점 천에 전달되서 불이붙었다.', '전기가 다리미를뜨겁게하고 뜨거우니까 탄다', '다리미를 계속 두고 있으면 뜨거워 지기 떄문에', '전기를 이용해서 뜨거워지기 때문에 ', '뜨거운 열에 인해 타게 되었을 것입니다.', '다라미는 뜨거운 열로 옷의 주름을 펴는 도구 이므로 뜨거운 열이 한 곳에 계속 가해지면서 다리미판에 불이 난 것 같다.', '뜨거워서', '뜨거운 다리미가 천으로 된 다리미판의 온도보다 훨씬 높아서 불이붙게 된 것이다. ', '다리미의 온도 때문에 타게된것같다.', ' 다리미에 열이 가해졌기 때문입니다.', '다리미가 전기로 뜨거운 열기를 만들기 때문', '불은 시간이 지날수록 더 뜨거워 지기 떄문이다.\\n', '다리미판이 천이니까', '다리미가 뜨거워 다리미 판이 가열이 됬기 때문이다.', '열이  가열되기 때문', '다리미가 뜨거운 상태로 다리미 바닥 부분을 세워놓지 않고 천이 있는 부분에 놓아  다리미의 뜨거운 열이 천에 불을 붙게 한 것 같다.', '뜨거운 다리미가 놓여있던 다리미판이 가열되면서 불이났기 때문입니다.', '천은 원래 불이 잘 붙으며, 게다가 다리미를 그냥 두고 갔기 때문에 다리미는 전기를 통해서 쓰므로 불이 날 수 있는 경우가 많다.', '발화점 이상의 온도, 탈 물질, 산소가 모두 만족되어었기 때문에', '.', '열', '다리미가 뜨거워서', '뜨거워서', '온도가 높아져서 ']\n",
      "['공기 땜에', '뜨거운 다리미에 있던 열이 천으로 전달되면서 타게 된 것이다.', '다리미가 뜨거워서', '다리미판이 불이 날 정도로 뜨거워지면 불이 나기 떄문에', '전기 때문에', '다리미의 열이 점점 뜨거워져서 불이 붙는 온도를 넘어서자 천인 다리미판이 탈 물질이 되어 불이 난 것이다.', '뜨거운다리미에 압력에 위하여 불씨가 붙어 불에 탄것이다', '물체가 분해되지 않아 시체가 쌓일것이다.', '다리미가  뜨거워서 천이 탓기 때문', '다리미가 뜨거워 지는 상태에서 천을 모래동안 붙여놓면 불이 날수있다. ', '다리미가 계속 열을 내기때문에 천에 붙어 다리미판이 타게된다.', '\\n뜨거우니까', '다리미 판이 뜨거워서 천이 불에 붙었다.', '다리미의 전력이 불을 불러일으켰기 때문이다', '수증기때문이다', '천에 불이 붙었는데 다리미판위에 있어서 불이 옴겨 붙었기 때문입니다.', '다리미에서 나오는 뜨거운 열이 천에 불이 붙여서 다리미판에 불이 났다', '불이 잘붙는 소질이 아니여서 ', '다리미판이뜨거워서 천이붛에붙었다', '다리미가 전기에 의해서 뜨거워졌기 때문', '다리미가 뜨거워서', '천에서 다리미판으로 불이 전달되어서', '열이 흡수되어서', '열이다식지안아서', '뜨거워서 안에서탐', '다리미 판이 가열되어서 ', '다리미에 열이 점점 천에 전달되서 불이붙었다.', '전기가 다리미를뜨겁게하고 뜨거우니까 탄다', '다리미를 계속 두고 있으면 뜨거워 지기 떄문에', '전기를 이용해서 뜨거워지기 때문에 ', '뜨거운 열에 인해 타게 되었을 것입니다.', '다라미는 뜨거운 열로 옷의 주름을 펴는 도구 이므로 뜨거운 열이 한 곳에 계속 가해지면서 다리미판에 불이 난 것 같다.', '뜨거워서', '뜨거운 다리미가 천으로 된 다리미판의 온도보다 훨씬 높아서 불이붙게 된 것이다. ', '다리미의 온도 때문에 타게된것같다.', ' 다리미에 열이 가해졌기 때문입니다.', '다리미가 전기로 뜨거운 열기를 만들기 때문', '불은 시간이 지날수록 더 뜨거워 지기 떄문이다.\\n', '다리미판이 천이니까', '다리미가 뜨거워 다리미 판이 가열이 됬기 때문이다.', '열이  가열되기 때문', '다리미가 뜨거운 상태로 다리미 바닥 부분을 세워놓지 않고 천이 있는 부분에 놓아  다리미의 뜨거운 열이 천에 불을 붙게 한 것 같다.', '뜨거운 다리미가 놓여있던 다리미판이 가열되면서 불이났기 때문입니다.', '천은 원래 불이 잘 붙으며 게다가 다리미를 그냥 두고 갔기 때문에 다리미는 전기를 통해서 쓰므로 불이 날 수 있는 경우가 많다.', '발화점 이상의 온도 탈 물질 산소가 모두 만족되어었기 때문에', '열', '다리미가 뜨거워서', '뜨거워서', '온도가 높아져서 ', '뜨거운 다리미가 판에 지속적인 열을 주어서']\n",
      "전처리 후 샘플 수 :  1245\n"
     ]
    }
   ],
   "source": [
    "# 전처리 \n",
    "import re\n",
    "\n",
    "## 특수 문자 제거\n",
    "answers_edited_29_more = []\n",
    "for ans in more_answers:\n",
    "    if ans==ans:\n",
    "        #print(ans)\n",
    "        n_ans = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣\\s\\t\\.]\", \"\", ans)\n",
    "        answers_edited_29_more.append(n_ans)\n",
    "\n",
    "print(len(answers_edited_29_more))\n",
    "\n",
    "\n",
    "## 빈칸인 답변은 지우기\n",
    "answers_removed_29_more = []\n",
    "for ans in answers_edited_29_more:\n",
    "    if not re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣]\", \"\", ans):\n",
    "        continue\n",
    "    elif ans:\n",
    "        answers_removed_29_more.append(ans)\n",
    "        \n",
    "print(more_answers[:50])\n",
    "print(answers_removed_29_more[:50])\n",
    "print(\"전처리 후 샘플 수 : \", len(answers_removed_29_more)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9c419ec-9864-4804-b30e-6a59c9aea418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['공기', '땜'],\n",
       " ['뜨거운', '다리미', '있', '열', '이', '천', '전달', '되', '타', '된', '것', '이'],\n",
       " ['다리미', '뜨거워서'],\n",
       " ['다리미판', '불', '날', '정도', '뜨거워지', '불', '나', '떄문에'],\n",
       " ['전기', '때문']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토큰화\n",
    "# docs에서는 띄어쓰기로 하지만, \n",
    "# 띄어쓰기 아예안된 문장도 많아서 우선 Mecab 사용\n",
    "\n",
    "# 1. 띄어쓰기 토큰화\n",
    "#tokenized_ans_29 = [ans.split() for ans in answers_removed_29] # 토큰화\n",
    "#tokenized_ans_29[:10]\n",
    "\n",
    "# 2. Mecab \n",
    "tokenized_ans_29_more =[]\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer= Mecab()\n",
    "for ans in answers_removed_29_more:\n",
    "    pos = tokenizer.pos(ans)\n",
    "    pos_cleaned = [p[0] for p in pos if p[1][0] not in ['J', 'S', 'E']]\n",
    "    tokenized_ans_29_more.append(pos_cleaned)\n",
    "    \n",
    "    \n",
    "# 3. Mecab tokenizing + subword segmentation(?)\n",
    "\n",
    "\n",
    "# ++ 불용어 제거도 해야할 듯\n",
    "\n",
    "print(sum([len(a) for a in tokenized_ans_29_more]))\n",
    "tokenized_ans_29_more[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e98b9fa-9d93-4c92-9378-a766e1bc21a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(tokenized_ans_29_more)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_ans_29_more]\n",
    "print(corpus[1]) # 수행된 결과에서 두번째 뉴스 출력. 첫번째 문서의 인덱스는 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f4958fb-6018-4b3e-bb39-8778cc6b6491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.075*\"온도\" + 0.054*\"때문\" + 0.050*\"발화점\" + 0.047*\"이\" + 0.041*\"불\" + 0.039*\"되\" + 0.038*\"다리미\" + 0.031*\"다리미판\" + 0.030*\"이상\" + 0.027*\"붙\"')\n",
      "(1, '0.077*\"다리미\" + 0.065*\"열\" + 0.041*\"이\" + 0.034*\"때문\" + 0.030*\"있\" + 0.029*\"천\" + 0.029*\"뜨거운\" + 0.024*\"불\" + 0.024*\"뜨거워서\" + 0.020*\"다리미판\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 2# k=2, 정답  & 오답\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a7cbf3e-4208-47a3-a012-1911a3643c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/pyLDAvis/_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el10802140168243084640403991320\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el10802140168243084640403991320_data = {\"mdsDat\": {\"x\": [0.11250454124959831, -0.11250454124959831], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [61.347349555235034, 38.652650444764966]}, \"tinfo\": {\"Term\": [\"\\uc628\\ub3c4\", \"\\ubc1c\\ud654\\uc810\", \"\\uc5f4\", \"\\ub418\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ub728\\uac70\\uc6cc\\uc11c\", \"\\uc774\\uc0c1\", \"\\uc0b0\\uc18c\", \"\\ub728\\uac70\\uc6b4\", \"\\ub728\\uac81\", \"\\uadf8\", \"\\ubb3c\\uc9c8\", \"\\ucc9c\", \"\\ud0c8\", \"\\ubaa8\\ub974\", \"\\uc798\", \"\\uac83\", \"\\uc5f4\\uae30\", \"\\ub3c4\\ub2ec\", \"\\uc9c0\", \"\\uac19\", \"\\uc804\\uae30\", \"\\uc774\\ub3d9\", \"\\uc62c\\ub77c\\uac00\", \"\\uc870\\uac74\", \"\\uc788\", \"\\ud574\\uc11c\", \"\\ub192\", \"\\ud0c4\", \"\\uacc4\\uc18d\", \"\\ubc1c\\ud654\\uc810\", \"\\uc774\\uc0c1\", \"\\uc870\\uac74\", \"\\uc62c\\ub77c\\uac14\", \"\\uc0b0\\uc18c\", \"\\uc628\\ub3c4\", \"\\ub3c4\\ub2ec\", \"\\uc62c\\ub790\", \"\\uc62c\\ub77c\\uac00\", \"\\ub418\", \"\\uc138\", \"\\ub118\", \"\\ucda9\\uc871\", \"\\uacf5\\uae09\", \"\\ucda9\\ubd84\", \"\\ubb3c\\uc9c8\", \"\\ubd99\\uc774\", \"\\ub9cc\\uc871\", \"\\uc84c\", \"\\uc874\\uc7ac\", \"\\ubaa8\\ub450\", \"\\uc804\\ub3c4\", \"\\ud0c8\", \"\\ud544\\uc694\", \"\\uadf8\\ub9ac\\uace0\", \"\\uac00\\uc9c0\", \"\\uc0c1\\ud669\", \"\\uc77c\\uc815\", \"\\uc704\\ud574\\uc11c\", \"\\uc804\\ub2ec\", \"\\uc5f0\\uc18c\", \"\\ud588\", \"\\ub192\", \"\\ubd99\", \"\\ub54c\\ubb38\", \"\\ubd88\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\uc774\", \"\\ud558\", \"\\uac00\\uc5f4\", \"\\uc788\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ud0c0\", \"\\uc5f4\", \"\\uc54a\", \"\\ub41c\", \"\\ucc9c\", \"\\ub728\\uac70\\uc6b4\", \"\\uac83\", \"\\ub728\\uac70\\uc6cc\\uc11c\", \"\\ubaa8\\ub974\", \"\\uc804\\uae30\", \"\\ud0c4\", \"\\uac19\", \"\\ub728\\uac70\\uc6cc\", \"\\uc798\", \"\\ub728\\uac70\\uc6b0\", \"\\ubaa8\\ub984\", \"\\uc5f4\\uae30\", \"\\ub0a8\", \"\\uc774\\ub3d9\", \"\\uadf8\", \"\\uc2dc\\uac04\", \"\\ub728\\uac81\", \"\\uc62e\\uaca8\", \"\\uac00\\ud574\", \"\\uc548\", \"\\ubd80\\ubd84\", \"\\ub09c\", \"\\uc131\\uc9c8\", \"\\ub728\\uac70\\uc6cc\\uc9c0\", \"\\uac74\\uc870\", \"\\ubd84\\uc790\", \"\\uc218\\ubd84\", \"\\uac00\\ud574\\uc838\\uc11c\", \"\\uc624\\ub79c\", \"\\ubab0\\ub77c\", \"\\ubab0\\ub77c\\uc694\", \"\\uc2dd\", \"\\uc9c0\", \"\\uc5d0\\ub108\\uc9c0\", \"\\ub0a0\", \"\\ub728\\uac70\\uc6cc\\uc838\\uc11c\", \"\\uc810\\uc810\", \"\\uac00\", \"\\ud0c0\\ub294\\uc810\", \"\\uac70\", \"\\uc5f4\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ud574\\uc11c\", \"\\ub728\\uac70\\uc6b4\", \"\\uacc4\\uc18d\", \"\\ucc9c\", \"\\uac83\", \"\\uc9c0\\uc18d\", \"\\ud310\", \"\\ub108\\ubb34\", \"\\uc788\", \"\\uc774\", \"\\ud0c0\", \"\\ub54c\\ubb38\", \"\\ubd88\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ud558\", \"\\ubd99\", \"\\uc218\"], \"Freq\": [457.0, 303.0, 395.0, 242.0, 521.0, 90.0, 179.0, 102.0, 179.0, 46.0, 40.0, 102.0, 195.0, 93.0, 27.0, 28.0, 96.0, 27.0, 57.0, 30.0, 23.0, 22.0, 23.0, 50.0, 45.0, 245.0, 39.0, 138.0, 18.0, 35.0, 302.4166164685291, 178.4977001284873, 44.735527528195355, 35.62164187591899, 100.47525546603346, 448.81863537516267, 55.990383308069525, 23.967525076439994, 49.67460116894854, 237.38421477198378, 21.24862633962825, 27.414028709650946, 18.53428902438406, 20.271320398062905, 18.47461778267337, 99.18741831420068, 16.611813091909998, 13.108725409527016, 26.155093173304934, 13.070455713186458, 15.677156091661749, 24.344263463960687, 89.23817171329286, 15.454478841736421, 11.158000894577105, 29.1830306736961, 7.689139288893145, 18.763624128032117, 7.674785875558048, 35.72944837705337, 37.803383124587945, 37.62010506586752, 122.69336676603855, 162.2168976983926, 325.46169738311147, 246.26272826865724, 183.75036134216194, 279.99344719766754, 98.23524573540469, 46.517988138241996, 131.59083783904563, 230.65105765467663, 81.88570947887506, 147.70178442110318, 37.60963131324203, 41.419036321206214, 85.47675201940439, 71.61949803131988, 35.71956599766953, 89.6114514642679, 27.390738846476207, 21.83038024954184, 18.15002991350599, 22.66716127358325, 16.258836530015568, 27.905641214761005, 14.381326115013211, 12.57218639529853, 26.017926626898316, 15.230841826704056, 22.32395777721725, 38.38299665560413, 12.463057282811013, 44.39453562070821, 9.759144839352016, 10.643600373525253, 11.522486302921907, 13.285162910373097, 10.593382987401581, 9.702933152843402, 6.9416417721600885, 8.652718304667038, 6.038044032603797, 6.013996195619809, 10.288996316462484, 5.114404810165253, 5.1132463888800785, 5.111835445881826, 5.105216904758607, 27.632543396781283, 10.150335407690507, 7.608618720073843, 13.27272312570327, 20.35278388023889, 10.789326592605125, 9.18399396892155, 12.949991546957728, 247.94238897621622, 291.18187615421203, 30.73054334473476, 108.08127099026775, 26.900089157963627, 109.65787529584486, 61.12865509834675, 15.609179393024046, 33.59280809292758, 23.550731215874602, 113.43789577544439, 153.5863335331978, 64.85502795806278, 129.1163367532963, 90.40701140894984, 75.54304654774143, 38.678810832444334, 28.709624436165928, 20.273172436946982], \"Total\": [457.0, 303.0, 395.0, 242.0, 521.0, 90.0, 179.0, 102.0, 179.0, 46.0, 40.0, 102.0, 195.0, 93.0, 27.0, 28.0, 96.0, 27.0, 57.0, 30.0, 23.0, 22.0, 23.0, 50.0, 45.0, 245.0, 39.0, 138.0, 18.0, 35.0, 303.24208379139657, 179.23672169773113, 45.27159266185542, 36.22207995945976, 102.33378216393072, 457.342058148018, 57.05373877696381, 24.452062150198564, 50.71720902125416, 242.7320489761275, 21.73679622739331, 28.077761890340817, 19.021387902226166, 20.833767649402603, 19.023144443475942, 102.37304594542175, 17.214393416032447, 13.590459139093188, 27.183331127427472, 13.591588513938861, 16.310205621670942, 25.37304365548548, 93.3491002657777, 16.31693286981824, 11.78432361257124, 30.821864474525533, 8.1593570129584, 19.946896066160576, 8.159744885727939, 38.08404338497778, 40.819140900423925, 40.824637170054686, 138.03298541095208, 190.92652213455852, 454.5780341364078, 336.6697396776071, 259.2934078899034, 433.5797807308653, 136.91405656784903, 57.342567115170176, 245.02873361449002, 521.8329338088887, 146.74073743693782, 395.6441733973194, 49.219724991741316, 57.49825044834784, 195.13462731524925, 179.70076902158763, 96.84822109601627, 90.4514538870651, 27.964656373838793, 22.369216608831284, 18.639694927802736, 23.299029262135445, 16.773403770531793, 28.884749040464392, 14.907415718364552, 13.043609270513468, 27.018798949795492, 15.837662285688452, 23.288914773458153, 40.05682221136683, 13.040381381048396, 46.57080351383569, 10.245022813970404, 11.176229372022707, 12.107331185104647, 13.969836710701369, 11.174668025135343, 10.243179103505895, 7.4461516386632, 9.306708837482304, 6.5142353800887856, 6.513418605648779, 11.165651793524695, 5.581773205211113, 5.581634710080974, 5.5815739321014375, 5.581469298918568, 30.686764775182024, 11.161365214140961, 8.370675224288359, 14.874384855195606, 23.230175725732053, 12.085425014916325, 10.227545449926158, 14.864573452437085, 395.6441733973194, 521.8329338088887, 39.82976448211289, 179.70076902158763, 35.19070478181895, 195.13462731524925, 96.84822109601627, 18.563913434387423, 49.870987180111534, 31.470947496897672, 245.02873361449002, 433.5797807308653, 146.74073743693782, 454.5780341364078, 336.6697396776071, 259.2934078899034, 136.91405656784903, 190.92652213455852, 52.19174437972406], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.9908, -3.5181, -4.9019, -5.1297, -4.0927, -2.596, -4.6775, -5.5259, -4.7971, -3.233, -5.6463, -5.3916, -5.783, -5.6934, -5.7862, -4.1056, -5.8925, -6.1294, -5.4386, -6.1323, -5.9504, -5.5103, -4.2113, -5.9647, -6.2905, -5.3291, -6.6628, -5.7707, -6.6647, -5.1267, -5.0702, -5.0751, -3.893, -3.6137, -2.9174, -3.1962, -3.4891, -3.0679, -4.1153, -4.8628, -3.8229, -3.2617, -4.2973, -3.7074, -5.0754, -4.9789, -4.2544, -4.4313, -5.1269, -3.7452, -4.9305, -5.1574, -5.342, -5.1198, -5.4521, -4.9119, -5.5748, -5.7092, -4.9819, -5.5174, -5.135, -4.5931, -5.7179, -4.4476, -5.9625, -5.8757, -5.7964, -5.6541, -5.8805, -5.9683, -6.3032, -6.0828, -6.4426, -6.4466, -5.9096, -6.6086, -6.6089, -6.6091, -6.6104, -4.9217, -5.9232, -6.2114, -5.655, -5.2275, -5.8621, -6.0232, -5.6796, -2.7275, -2.5668, -4.8154, -3.5578, -4.9486, -3.5433, -4.1277, -5.4928, -4.7264, -5.0815, -3.5094, -3.2064, -4.0686, -3.38, -3.7364, -3.916, -4.5854, -4.8835, -5.2314], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4859, 0.4845, 0.4767, 0.4719, 0.4703, 0.4698, 0.4698, 0.4686, 0.4678, 0.4663, 0.4659, 0.4647, 0.4627, 0.4613, 0.4594, 0.457, 0.453, 0.4525, 0.4501, 0.4495, 0.449, 0.4472, 0.4436, 0.4343, 0.434, 0.434, 0.4293, 0.4275, 0.4273, 0.4248, 0.4119, 0.4069, 0.3708, 0.3257, 0.1545, 0.1759, 0.1442, 0.0513, 0.1566, 0.2794, -0.1331, -0.3278, -0.0947, -0.4967, 0.2196, 0.1606, -0.3368, -0.4313, -0.5088, 0.9412, 0.9298, 0.9262, 0.9239, 0.9231, 0.9194, 0.9161, 0.9146, 0.9137, 0.9128, 0.9115, 0.9082, 0.9079, 0.9053, 0.9027, 0.902, 0.9017, 0.901, 0.9003, 0.8971, 0.8964, 0.8804, 0.8777, 0.8746, 0.8708, 0.8688, 0.8631, 0.8629, 0.8626, 0.8614, 0.8457, 0.8556, 0.8551, 0.8366, 0.8183, 0.8371, 0.8429, 0.8127, 0.4832, 0.3672, 0.6912, 0.4421, 0.6819, 0.3742, 0.4904, 0.7772, 0.5554, 0.6606, 0.1804, -0.0873, 0.134, -0.3081, -0.3642, -0.2827, -0.3135, -0.9441, 0.0049]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.08274429726432948, 0.9101872699076243, 0.819635436020896, 0.19182957013255011, 0.9408905169889603, 0.06488900117165244, 0.08947561531828306, 0.9842317685011137, 0.08956037842590889, 0.8956037842590889, 0.04292024310322473, 0.9871655913741688, 0.1345480922408907, 0.8745625995657895, 0.10744936985377151, 0.9670443286839436, 0.3717156556165265, 0.6298515275724477, 0.22733275873841402, 0.7672480607421474, 0.9599799871327397, 0.047998999356636984, 0.04992907299152814, 0.9486523868390346, 0.9334434764049977, 0.08485849785499978, 0.08948811702957846, 0.9843692873253631, 0.11946467557341123, 0.9557174045872898, 0.06314063161352039, 0.947109474202806, 0.2542027055521166, 0.7626081166563498, 0.9616151068397092, 0.035615374327396636, 0.8910913549671056, 0.10866967743501287, 0.4426704123749294, 0.5576497402645214, 0.7096208171945769, 0.2931042505803687, 0.9815307673160015, 0.017527335130642883, 0.9763852816292453, 0.020598845603992517, 0.7130651746844255, 0.27826933646221486, 0.7149487559763511, 0.28377966006445937, 0.06708070794377143, 0.9391299112128, 0.4006660649924685, 0.6009990974887027, 0.05961819161337076, 0.9538910658139321, 0.011055654243530118, 0.9950088819177105, 0.13445934198088214, 0.8739857228757338, 0.13429756047508173, 0.9400829233255721, 0.0429453616664746, 0.9447979566624413, 0.9565534075743827, 0.9809808883550322, 0.06131130552218951, 0.035759423846720664, 0.965504443861458, 0.996656656174756, 0.8957949166701137, 0.8958046710164282, 0.9670514253603432, 0.029304588647283126, 0.9959039860962998, 0.0032976953181996682, 0.07158279804616227, 0.9305763746001096, 0.9210597483688444, 0.7306863997802955, 0.2673242926025471, 0.8484939556266986, 0.15189089329119915, 0.9875456886077221, 0.05809092285927777, 0.9771944111262088, 0.019543888222524175, 0.980469415334405, 0.09762594111604801, 0.9762594111604801, 0.9661037339778353, 0.6131237876853118, 0.3832023673033199, 0.9211752480942166, 0.07668487376092399, 0.920218485131088, 0.8958214642457623, 0.08259458543847183, 0.9911350252616619, 0.7720481982858722, 0.2438046941955386, 0.08959477454720716, 0.8959477454720717, 0.9309358100578092, 0.07349493237298495, 0.37407349823745123, 0.6268258619114048, 0.03701126766804596, 0.9622929593691949, 0.8957726901788176, 0.9817596960537618, 0.01967892486522017, 0.9858586654294482, 0.019717173308588964, 0.993868934094665, 0.027607470391518472, 0.9815123097830465, 0.9760837219770477, 0.9804228088052918, 0.6457865713387673, 0.35518261423632197, 0.04293888357304125, 0.9446554386069076, 0.9931000651762826, 0.005579213849304958, 0.9525291522540711, 0.05013311327653006, 0.5387123299901594, 0.461170403703697, 0.03462034579559991, 0.9693696822767975, 0.04470429239820601, 0.9834944327605323, 0.9452777804102641, 0.052515432245014676, 0.9458857331375522, 0.03941190554739801, 0.12914237220671998, 0.8609491480447998, 0.9564685018962407, 0.036787250072932334, 0.9940008149506907, 0.02208890699890424, 0.9564739240499992, 0.07357491723461533, 0.09776201636042961, 0.9124454860306763, 0.16160385635298535, 0.8618872338825885, 0.43559670146436114, 0.5637133783656438, 0.9462158085107305, 0.05256754491726281, 0.9988755866640171, 0.5588086950649249, 0.4429581119417088, 0.09777517048405979, 0.879976534356538, 0.9656810409032728, 0.9534103676050952, 0.04284990416202675, 0.32082781803005445, 0.6817591133138656, 0.9192904156482622, 0.061286027709884144, 0.7157774917832136, 0.2848502263218911, 0.22596166753739658, 0.7783124104065882, 0.93081047705853, 0.07348503766251553], \"Term\": [\"\\uac00\", \"\\uac00\", \"\\uac00\\uc5f4\", \"\\uac00\\uc5f4\", \"\\uac00\\uc9c0\", \"\\uac00\\uc9c0\", \"\\uac00\\ud574\", \"\\uac00\\ud574\", \"\\uac00\\ud574\\uc838\\uc11c\", \"\\uac00\\ud574\\uc838\\uc11c\", \"\\uac19\", \"\\uac19\", \"\\uac70\", \"\\uac70\", \"\\uac74\\uc870\", \"\\uac74\\uc870\", \"\\uac83\", \"\\uac83\", \"\\uacc4\\uc18d\", \"\\uacc4\\uc18d\", \"\\uacf5\\uae09\", \"\\uacf5\\uae09\", \"\\uadf8\", \"\\uadf8\", \"\\uadf8\\ub9ac\\uace0\", \"\\uadf8\\ub9ac\\uace0\", \"\\ub09c\", \"\\ub09c\", \"\\ub0a0\", \"\\ub0a0\", \"\\ub0a8\", \"\\ub0a8\", \"\\ub108\\ubb34\", \"\\ub108\\ubb34\", \"\\ub118\", \"\\ub118\", \"\\ub192\", \"\\ub192\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ub2e4\\ub9ac\\ubbf8\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ub2e4\\ub9ac\\ubbf8\\ud310\", \"\\ub3c4\\ub2ec\", \"\\ub3c4\\ub2ec\", \"\\ub418\", \"\\ub418\", \"\\ub41c\", \"\\ub41c\", \"\\ub54c\\ubb38\", \"\\ub54c\\ubb38\", \"\\ub728\\uac70\\uc6b0\", \"\\ub728\\uac70\\uc6b0\", \"\\ub728\\uac70\\uc6b4\", \"\\ub728\\uac70\\uc6b4\", \"\\ub728\\uac70\\uc6cc\", \"\\ub728\\uac70\\uc6cc\", \"\\ub728\\uac70\\uc6cc\\uc11c\", \"\\ub728\\uac70\\uc6cc\\uc11c\", \"\\ub728\\uac70\\uc6cc\\uc838\\uc11c\", \"\\ub728\\uac70\\uc6cc\\uc838\\uc11c\", \"\\ub728\\uac70\\uc6cc\\uc9c0\", \"\\ub728\\uac70\\uc6cc\\uc9c0\", \"\\ub728\\uac81\", \"\\ub728\\uac81\", \"\\ub9cc\\uc871\", \"\\ubaa8\\ub450\", \"\\ubaa8\\ub450\", \"\\ubaa8\\ub974\", \"\\ubaa8\\ub974\", \"\\ubaa8\\ub984\", \"\\ubab0\\ub77c\", \"\\ubab0\\ub77c\\uc694\", \"\\ubb3c\\uc9c8\", \"\\ubb3c\\uc9c8\", \"\\ubc1c\\ud654\\uc810\", \"\\ubc1c\\ud654\\uc810\", \"\\ubd80\\ubd84\", \"\\ubd80\\ubd84\", \"\\ubd84\\uc790\", \"\\ubd88\", \"\\ubd88\", \"\\ubd99\", \"\\ubd99\", \"\\ubd99\\uc774\", \"\\ubd99\\uc774\", \"\\uc0b0\\uc18c\", \"\\uc0b0\\uc18c\", \"\\uc0c1\\ud669\", \"\\uc131\\uc9c8\", \"\\uc131\\uc9c8\", \"\\uc138\", \"\\uc218\", \"\\uc218\", \"\\uc218\\ubd84\", \"\\uc2dc\\uac04\", \"\\uc2dc\\uac04\", \"\\uc2dd\", \"\\uc548\", \"\\uc548\", \"\\uc54a\", \"\\uc54a\", \"\\uc5d0\\ub108\\uc9c0\", \"\\uc5d0\\ub108\\uc9c0\", \"\\uc5f0\\uc18c\", \"\\uc5f0\\uc18c\", \"\\uc5f4\", \"\\uc5f4\", \"\\uc5f4\\uae30\", \"\\uc5f4\\uae30\", \"\\uc624\\ub79c\", \"\\uc628\\ub3c4\", \"\\uc628\\ub3c4\", \"\\uc62c\\ub77c\\uac00\", \"\\uc62c\\ub77c\\uac00\", \"\\uc62c\\ub77c\\uac14\", \"\\uc62c\\ub77c\\uac14\", \"\\uc62c\\ub790\", \"\\uc62e\\uaca8\", \"\\uc704\\ud574\\uc11c\", \"\\uc774\", \"\\uc774\", \"\\uc774\\ub3d9\", \"\\uc774\\ub3d9\", \"\\uc774\\uc0c1\", \"\\uc774\\uc0c1\", \"\\uc77c\\uc815\", \"\\uc77c\\uc815\", \"\\uc788\", \"\\uc788\", \"\\uc798\", \"\\uc798\", \"\\uc804\\uae30\", \"\\uc804\\uae30\", \"\\uc804\\ub2ec\", \"\\uc804\\ub2ec\", \"\\uc804\\ub3c4\", \"\\uc804\\ub3c4\", \"\\uc810\\uc810\", \"\\uc810\\uc810\", \"\\uc84c\", \"\\uc84c\", \"\\uc870\\uac74\", \"\\uc870\\uac74\", \"\\uc874\\uc7ac\", \"\\uc874\\uc7ac\", \"\\uc9c0\", \"\\uc9c0\", \"\\uc9c0\\uc18d\", \"\\uc9c0\\uc18d\", \"\\ucc9c\", \"\\ucc9c\", \"\\ucda9\\ubd84\", \"\\ucda9\\ubd84\", \"\\ucda9\\uc871\", \"\\ud0c0\", \"\\ud0c0\", \"\\ud0c0\\ub294\\uc810\", \"\\ud0c0\\ub294\\uc810\", \"\\ud0c4\", \"\\ud0c8\", \"\\ud0c8\", \"\\ud310\", \"\\ud310\", \"\\ud544\\uc694\", \"\\ud544\\uc694\", \"\\ud558\", \"\\ud558\", \"\\ud574\\uc11c\", \"\\ud574\\uc11c\", \"\\ud588\", \"\\ud588\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el10802140168243084640403991320\", ldavis_el10802140168243084640403991320_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el10802140168243084640403991320\", ldavis_el10802140168243084640403991320_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el10802140168243084640403991320\", ldavis_el10802140168243084640403991320_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e55ec65-9c43-4d35-8b36-200c4c2fb7a1",
   "metadata": {},
   "source": [
    "## 02. Doc2Vec\n",
    "https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "857ec41a-2422-41b2-b51b-b4dffb91276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64342a-41f6-432f-a02b-2204d6441a59",
   "metadata": {},
   "source": [
    "We will first import the model and other libraries and then we will build a tagged sentence corpus. Each sentence is now represented as a TaggedDocument containing a list of the words in it and a tag associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2585b091-2aca-487a-85a2-161ae1b6f108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['공기', '땜'], tags=[0]),\n",
       " TaggedDocument(words=['뜨거운', '다리미', '있', '열', '이', '천', '전달', '되', '타', '된', '것', '이'], tags=[1]),\n",
       " TaggedDocument(words=['다리미', '뜨거워서'], tags=[2]),\n",
       " TaggedDocument(words=['다리미판', '불', '날', '정도', '뜨거워지', '불', '나', '떄문에'], tags=[3]),\n",
       " TaggedDocument(words=['전기', '때문'], tags=[4]),\n",
       " TaggedDocument(words=['다리미', '열', '점점', '뜨거워져서', '불', '붙', '온도', '넘어서', '천인', '다리미판', '탈', '물질', '되', '불', '난', '것', '이'], tags=[5]),\n",
       " TaggedDocument(words=['뜨거운', '다리미', '압력', '위하', '불씨', '붙', '불', '탄', '것', '이'], tags=[6]),\n",
       " TaggedDocument(words=['물체', '분해', '되', '않', '시체', '쌓일', '것', '이'], tags=[7]),\n",
       " TaggedDocument(words=['다리미', '뜨거워서', '천', '탓', '때문'], tags=[8]),\n",
       " TaggedDocument(words=['다리미', '뜨거워', '지', '상태', '천', '모래', '동안', '붙여', '놓', '불', '날', '수', '있'], tags=[9])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_ans_29_more)]\n",
    "tagged_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6921d333-de98-411c-a066-48c17d9fbc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Size: 0 / "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/past/builtins/misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  from imp import reload\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/multiclass.py:14: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe2` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n",
      "/opt/conda/lib/python3.8/site-packages/sklearn/utils/optimize.py:18: DeprecationWarning: Please use `line_search_wolfe1` from the `scipy.optimize` namespace, the `scipy.optimize.linesearch` namespace is deprecated.\n",
      "  from scipy.optimize.linesearch import line_search_wolfe2, line_search_wolfe1\n"
     ]
    }
   ],
   "source": [
    "## Train doc2vec model\n",
    "model = Doc2Vec(vector_size=300, alpha=0.025, min_alpha=0.025, workers=8, window=5)\n",
    "'''\n",
    "vector_size = Dimensionality of the feature vectors.\n",
    "window = The maximum distance between the current and predicted word within a sentence.\n",
    "min_count = Ignores all words with total frequency lower than this.\n",
    "alpha = The initial learning rate.\n",
    "'''\n",
    "\n",
    "# Vocabulary 빌드\n",
    "model.build_vocab(tagged_data)\n",
    "print(f\"Tag Size: {len(model.docvecs.doctags.keys())}\", end=' / ')\n",
    "\n",
    "# Doc2Vec 학습\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=50)\n",
    "\n",
    "# 모델 저장\n",
    "model.save('dart.doc2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c9a009e-a746-4069-aad1-f9cfeebd75bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npositive = List of sentences that contribute positively.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tokenizer.pos(ans)\n",
    "test_cleaned = [p[0] for p in pos if p[1][0] not in ['J', 'S', 'E']]\n",
    "#test_doc = word_tokenize(\"다리미 뜨겁다\".lower())\n",
    "test_doc_vector = model.infer_vector(test_cleaned)\n",
    "model.docvecs.most_similar(positive = [test_doc_vector])\n",
    "\n",
    "'''\n",
    "positive = List of sentences that contribute positively.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba69fa2-b331-4c3e-9184-5e7bb5a94836",
   "metadata": {},
   "source": [
    "## 03.K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6cf17-2be3-4fc5-a777-f3cc0262b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 2\n",
    "\n",
    "# 그룹 수, random_state 설정\n",
    "model = KMeans(n_clusters = k, random_state = 10)\n",
    "\n",
    "# 정규화된 데이터에 학습\n",
    "model.fit(data_scale)\n",
    "\n",
    "# 클러스터링 결과 각 데이터가 몇 번째 그룹에 속하는지 저장\n",
    "df['cluster'] = model.fit_predict(data_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9354ab-f4bc-4136-b27c-c2c8a0abd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 8))\n",
    "\n",
    "for i in range(k):\n",
    "    plt.scatter(df.loc[df['cluster'] == i, 'Annual Income (k$)'], df.loc[df['cluster'] == i, 'Spending Score (1-100)'], \n",
    "                label = 'cluster ' + str(i))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('K = %d results'%k , size = 15)\n",
    "plt.xlabel('Annual Income', size = 12)\n",
    "plt.ylabel('Spending Score', size = 12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
